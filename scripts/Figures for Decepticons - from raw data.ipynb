{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dfe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import breaching\n",
    "except ModuleNotFoundError:\n",
    "    # You only really need this safety net if you want to run these notebooks directly in the examples directory\n",
    "    # Don't worry about this if you installed the package or moved the notebook to the main directory.\n",
    "    import os; os.chdir(\"..\")\n",
    "    import breaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_TO_FILE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(error_y_mode=None,  exponential_trendline=False, **kwargs):\n",
    "    \"\"\"Extension of `plotly.express.line` to use error bands.\"\"\"\n",
    "    ERROR_MODES = {'bar','band','bars','bands',None}\n",
    "    if error_y_mode not in ERROR_MODES:\n",
    "        raise ValueError(f\"'error_y_mode' must be one of {ERROR_MODES}, received {repr(error_y_mode)}.\")\n",
    "    if error_y_mode in {'bar','bars',None}:\n",
    "        fig = px.line(**kwargs)\n",
    "    elif error_y_mode in {'band','bands'}:\n",
    "        if 'error_y' not in kwargs:\n",
    "            raise ValueError(f\"If you provide argument 'error_y_mode' you must also provide 'error_y'.\")\n",
    "        figure_with_error_bars = px.line(**kwargs)\n",
    "        fig = px.line(**{arg: val for arg,val in kwargs.items() if arg != 'error_y'})\n",
    "        for data in figure_with_error_bars.data:\n",
    "            x = list(data['x'])\n",
    "            y_upper = list(data['y'] + data['error_y']['array'])\n",
    "            y_lower = list(data['y'] - data['error_y']['array'] if data['error_y']['arrayminus'] is None else data['y'] - data['error_y']['arrayminus'])\n",
    "            color = f\"rgba({tuple(int(data['line']['color'].lstrip('#')[i:i+2], 16) for i in (0, 2, 4))},.3)\".replace('((','(').replace('),',',').replace(' ','')\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = x+x[::-1],\n",
    "                    y = y_upper+y_lower[::-1],\n",
    "                    fill = 'toself',\n",
    "                    fillcolor = color,\n",
    "                    line = dict(\n",
    "                        color = 'rgba(255,255,255,0)'\n",
    "                    ),\n",
    "                    hoverinfo = \"skip\",\n",
    "                    showlegend = False,\n",
    "                    legendgroup = data['legendgroup'],\n",
    "                    xaxis = data['xaxis'],\n",
    "                    yaxis = data['yaxis'],\n",
    "                )\n",
    "            )\n",
    "            if exponential_trendline:\n",
    "                coeffs = fit_exponential(data['x'], data['y'])\n",
    "                xeval = np.logspace(np.log10(min(data['x'])), np.log10(max(data['x'])), num=500)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=xeval, y=[f(xx, *coeffs) for xx in xeval], mode='lines', \n",
    "                        line=dict(width=2, color=color),\n",
    "                        hoverinfo = \"skip\",\n",
    "                        showlegend = False,\n",
    "                        legendgroup = data['legendgroup'],\n",
    "                        xaxis = data['xaxis'],\n",
    "                        yaxis = data['yaxis'])\n",
    "                )\n",
    "        # Reorder data as said here: https://stackoverflow.com/a/66854398/8849755\n",
    "        reordered_data = []\n",
    "        for i in range(int(len(fig.data)/2)):\n",
    "            reordered_data.append(fig.data[i+int(len(fig.data)/2)])\n",
    "            reordered_data.append(fig.data[i])\n",
    "        fig.data = tuple(reordered_data)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c209559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_styling(fig):\n",
    "    \n",
    "    fig.update_layout(\n",
    "        font=dict(\n",
    "            family=\"Times New Roman\",\n",
    "            size=20,      \n",
    "            ),\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.24,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "            bgcolor=\"white\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(uniformtext_minsize=28, uniformtext_mode='hide',font=dict(family=\"Times New Roman\"))\n",
    "    \n",
    "\n",
    "    fig.update_xaxes(showline=True, showgrid=True, gridwidth=0.1, \n",
    "                     gridcolor='rgba(1,1,1,0.15)', \n",
    "                     linecolor='black',\n",
    "                    zerolinecolor='rgba(1,1,1,0.25)')\n",
    "    fig.update_yaxes(showline=True, showgrid=True, gridwidth=0.1, \n",
    "                     gridcolor='rgba(1,1,1,0.15)', \n",
    "                     linecolor='black',\n",
    "                     zerolinecolor='rgba(1,1,1,0.25)')\n",
    "    fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "    )\n",
    ")\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=20, r=30, t=20, b=0),\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        width=750,\n",
    "        height=500,\n",
    "    )    \n",
    "    \n",
    "#     fig.show()\n",
    "#     fig.write_image(f\"images/{key}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65513ee6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.listdir(logfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"UNDIV\", \"S1\", \"S25\", \"S2\", \"S3\", \"SBBF\", \"TFirst\", \"tokenreplace\", \n",
    "              \"noEXT_trafo3_baseparams\", \"baseparams\", \"noEXT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_name(name):\n",
    "    for exp in experiments:\n",
    "        if exp in name:\n",
    "            return exp\n",
    "    return \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51734a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_table(output_folder, name):\n",
    "        file = os.listdir(os.path.join(output_folder, \"tables\"))[-1]\n",
    "        file_location = os.path.join(output_folder, \"tables\", file)\n",
    "        # print(file_location)\n",
    "\n",
    "        df = pd.read_csv(file_location, sep='\\t')\n",
    "        df[\"dataset\"] = file_location.split(\"training_\")[1].split(\"_reports.csv\")[0]\n",
    "        df[\"seq_length\"] = [int(vals) for vals in name.split(\"_\") if vals.isdigit()][0]\n",
    "        df[\"experiment\"] = df[\"name\"].map(simplify_name)   \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(folderlist):\n",
    "    dfs = dict()\n",
    "    for name in folderlist:\n",
    "        if \"decepticon\" in name:\n",
    "            output_folder = logfiles + name\n",
    "            candidate_runs = sorted(glob.glob(output_folder + '/*/*'), reverse=True)\n",
    "            # print(candidate_runs)\n",
    "            for candidate in candidate_runs:\n",
    "            # lastdate = sorted(os.listdir(output_folder))[-1]\n",
    "                if \"2022-07-\" in candidate or \"2022-08-\" in candidate:\n",
    "                    # print(candidate)\n",
    "                    # output_folder = os.path.join(output_folder, lastdate)\n",
    "                    # lasttimestamp = sorted(os.listdir(output_folder))[-1]\n",
    "                    # output_folder = os.path.join(output_folder, candidate)\n",
    "                    try:\n",
    "                        df = _retrieve_table(candidate, name)\n",
    "                        dfs[name] = df\n",
    "                        break\n",
    "                    except Exception as e :\n",
    "                        if \"No such file or directory\" not in str(e):\n",
    "                            print(f\"skipped {name} with exception {e}\")\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758ab43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = get_result(os.listdir(\"outputs\"))\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"max-sentence_accuracy\" # \"max-sentence_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b06448",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = pd.DataFrame()\n",
    "summaries = []\n",
    "for df in dfs.values():\n",
    "    try:\n",
    "        summary = (\n",
    "        df\n",
    "        .groupby([\"name\", \"datapoints\", \"seq_length\", \"experiment\", \"dataset\", \"model\"], as_index=False)[metric]\n",
    "        .agg([\"mean\", \"std\", \"count\"])\n",
    "        .reset_index()\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "main_table = pd.concat(summaries)\n",
    "main_table[\"tokens_recovered\"] = main_table[\"datapoints\"] * main_table[\"seq_length\"] * main_table[\"mean\"]\n",
    "main_table = main_table.sort_values(\"seq_length\")\n",
    "main_table = main_table.sort_values(\"datapoints\")\n",
    "# main_table = main_table[~main_table.name.str.contains(\"gpt_sanity\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table[(main_table.seq_length==512) & (main_table.dataset==\"stackoverflow\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.dataset==\"wikitext\") & (main_table.seq_length == seq_length)]\n",
    "\n",
    "fig = px.scatter(reduced_table, x=\"datapoints\", y=\"mean\", title='Hyperparameter Variations', \n",
    "                 color=\"experiment\", symbol=\"model\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=3, dash='solid'))\n",
    "\n",
    "\n",
    "# default_styling(fig)\n",
    "fig.update_xaxes(type=\"log\")\n",
    "# fig.update_layout(legend=dict(y=0.39, x=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d36aed",
   "metadata": {},
   "source": [
    "# Model Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56be649",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0728b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.dataset==\"wikitext\") \n",
    "                           & (main_table.seq_length == seq_length)]\n",
    "\n",
    "# reduced_table = reduced_table[~reduced_table.name.str.contains(\"gpt_sanity\")]\n",
    "\n",
    "reduced_table = reduced_table.groupby([\"model\", \"datapoints\", \"seq_length\"], as_index=False).mean().reset_index()\n",
    "df_t = reduced_table#[reduced_table.experiment.isin([\"base\"])]\n",
    "\n",
    "df_t = df_t.replace({\n",
    "                        'gpt2S':\"GPT-2\", \n",
    "                        'transformer3':\"3-layer Transformer\", \n",
    "                        'bert-sanity-check':\"BERT-base\",  \n",
    "                        }\n",
    "                         )\n",
    "\n",
    "fig = px.scatter(data_frame=df_t, x=\"datapoints\", y=\"mean\", color=\"model\",  #error_y=\"std\", error_y_mode=\"band\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on Most Leaked Sequence\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "                \"model\": \"Model\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "#fig.update_xaxes(range=[2,])\n",
    "fig.update_layout(legend=dict(y=0.31, x=0.315))\n",
    "fig.show()\n",
    "\n",
    "if WRITE_TO_FILE:\n",
    "    fig.write_image(f\"images/decepticon_model_scaling_{metric}_{seq_length}.pdf\")\n",
    "    fig.write_image(f\"images/decepticon_model_scaling_{metric}_{seq_length}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73089132",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.dataset==\"stackoverflow\") \n",
    "                           & (main_table.seq_length == seq_length)]\n",
    "\n",
    "# reduced_table = reduced_table[~reduced_table.name.str.contains(\"gpt_sanity\")]\n",
    "reduced_table = reduced_table.groupby([\"model\", \"datapoints\", \"seq_length\"], as_index=False).mean().reset_index()\n",
    "\n",
    "\n",
    "df_t = reduced_table# [reduced_table.experiment.isin([\"base\"])]\n",
    "\n",
    "df_t = df_t.replace({\n",
    "                        'gpt2S':\"GPT-2\", \n",
    "                        'transformer3':\"3-layer Transformer\", \n",
    "                        'bert-sanity-check':\"BERT-base\",  \n",
    "                        }\n",
    "                         )\n",
    "\n",
    "fig = px.scatter(data_frame=df_t, x=\"datapoints\", y=\"mean\", color=\"model\",  #error_y=\"std\", error_y_mode=\"band\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on Most Leaked Sequence\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "                \"model\": \"Model\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "#fig.update_xaxes(range=[2,])\n",
    "fig.update_layout(legend=dict(y=0.31, x=0.315))\n",
    "fig.show()\n",
    "\n",
    "if WRITE_TO_FILE:\n",
    "    fig.write_image(f\"images/decepticon_model_scaling_{metric}_{seq_length}.pdf\")\n",
    "    fig.write_image(f\"images/decepticon_model_scaling_{metric}_{seq_length}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aabde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = line(data_frame=df_t, x=\"datapoints\", y=\"mean\", color=\"model\",  error_y=\"std\", error_y_mode=\"band\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on Most Leaked Sequence\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "#fig.update_xaxes(range=[2,])\n",
    "fig.update_layout(legend=dict(y=0.39, x=0.257))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae1c7b",
   "metadata": {},
   "source": [
    "# Threshold Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.dataset==\"wikitext\") \n",
    "                           & (main_table.seq_length == seq_length) \n",
    "                           & (main_table.model==\"gpt2S\")]\n",
    "df_t = reduced_table[reduced_table.experiment.isin([\"S1\", \"S25\", \"S2\", \"base\"])]\n",
    "\n",
    "df_t = df_t.replace({\n",
    "                        'S1':\"sd=1\", \n",
    "                        'base':\"sd=1.5\", \n",
    "                        'S2':\"sd=2\", \n",
    "                        'S25':\"sd=2.5\", \n",
    "                        }\n",
    "                         )\n",
    "\n",
    "fig = px.scatter(data_frame=df_t, x=\"datapoints\", y=\"mean\", color=\"experiment\",  #\n",
    "                 #error_y=\"std\", error_y_mode=\"band\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on most Leaked Sequence\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "fig.data[1].line.dash=\"dash\"\n",
    "fig.data[2].line.dash=\"dashdot\"\n",
    "fig.data[3].line.dash=\"dot\"\n",
    "\n",
    "default_styling(fig)\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "#fig.update_xaxes(range=[2,])\n",
    "fig.update_layout(legend=dict(y=0.39, x=0.2))\n",
    "fig.show()\n",
    "\n",
    "if WRITE_TO_FILE:\n",
    "    fig.write_image(f\"images/decepticon_embedding_threshold_{metric}.pdf\")\n",
    "    fig.write_image(f\"images/decepticon_embedding_threshold_{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b27a8",
   "metadata": {},
   "source": [
    "# No External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a005dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.dataset==\"wikitext\") \n",
    "                           & (main_table.seq_length == seq_length) \n",
    "                           & (main_table.model==\"gpt2S\")]\n",
    "\n",
    "reduced_table = reduced_table[~reduced_table.name.str.contains(\"gptS\")]\n",
    "\n",
    "df_ext = reduced_table[reduced_table.experiment.isin([\"noEXT\", \"base\"])]\n",
    "\n",
    "df_ext = df_ext.replace({\n",
    "                        'base':\"Estimated on public Data\", \n",
    "                        'noEXT':\"Estimated on random data\", \n",
    "                        }\n",
    "                         )\n",
    "\n",
    "fig = px.scatter(data_frame=df_ext, x=\"datapoints\", y=\"mean\", color=\"experiment\", \n",
    "                 # error_y=\"std\", error_y_mode=\"band\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on Most Leaked Sequence\",  \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "fig.data[1].line.dash=\"dash\"\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(legend=dict(y=0.25, x=0.4))\n",
    "fig.show()\n",
    "if WRITE_TO_FILE:\n",
    "    fig.write_image(f\"images/decepticon_ext_vs_simualated_{metric}.pdf\")\n",
    "    fig.write_image(f\"images/decepticon_ext_vs_simualated_{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad552f36",
   "metadata": {},
   "source": [
    "# Dataset variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f77f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt2S\"\n",
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71981ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.seq_length == seq_length) \n",
    "                           & (main_table.model==model) & (main_table.experiment==\"base\")]\n",
    "reduced_table = reduced_table[~reduced_table.name.str.contains(\"gptS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(reduced_table, x=\"datapoints\", y=\"mean\", color=\"dataset\", #symbol=\"is_base\",\n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\" if metric==\"accuracy\" else \"Accuracy on Most Leaked Sequence\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "                \"dataset\":\"Dataset\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(legend=dict(y=0.39, x=0.257))\n",
    "fig.show()\n",
    "\n",
    "if WRITE_TO_FILE:\n",
    "    fig.write_image(f\"images/decepticon_data_sources_{metric}.pdf\")\n",
    "    fig.write_image(f\"images/decepticon_data_sources_{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e805dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = line(data_frame=reduced_table, x=\"datapoints\", y=\"mean\", color=\"dataset\", \n",
    "           error_y=\"std\", error_y_mode=\"band\", \n",
    "                                 labels={ # replaces default labels by column name\n",
    "                \"experiment\": \"Experiment\",  \n",
    "                \"mean\": f\"Total Accuracy\", \n",
    "                \"real_size\": \"Real Dataset Samples\",\n",
    "                \"datapoints\": \"Batch Size\",\n",
    "                \"dataset\":\"Dataset\",\n",
    "            })\n",
    "fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "default_styling(fig)\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(legend=dict(y=0.39, x=0.257))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b38bb",
   "metadata": {},
   "source": [
    "# Print recovered total tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baeaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"transformer3\"\n",
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43095e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_table = main_table[(main_table.seq_length == seq_length) & (main_table.model==model) \n",
    "                           & (main_table.experiment==\"base\")]\n",
    "# reduced_table = reduced_table[~reduced_table.name.str.contains(\"gptS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric == \"accuracy\":\n",
    "    fig = px.scatter(reduced_table, x=\"datapoints\", y=\"tokens_recovered\", color=\"dataset\", # symbol=\"model\",\n",
    "                                     labels={ # replaces default labels by column name\n",
    "                    \"experiment\": \"Experiment\",  \n",
    "                    \"mean\": f\"Total Accuracy\", \n",
    "                    \"real_size\": \"Real Dataset Samples\",\n",
    "                    \"datapoints\": \"Batch Size\",\n",
    "                    \"dataset\":\"Dataset\",\n",
    "                    \"tokens_recovered\":\"Total number of tokens leaked exactly\"\n",
    "                })\n",
    "    fig.update_traces(mode='lines+markers', line=dict(width=5, dash='solid'))\n",
    "\n",
    "\n",
    "    default_styling(fig)\n",
    "    fig.update_xaxes(type=\"log\")\n",
    "    fig.update_yaxes(type=\"log\")\n",
    "    fig.update_layout(legend=dict(y=1, x=0.257))\n",
    "    fig.show()\n",
    "\n",
    "    if WRITE_TO_FILE:\n",
    "        fig.write_image(f\"images/decepticon_total_tokens_{model}_{seq_length}.pdf\")\n",
    "        fig.write_image(f\"images/decepticon_total_tokens_{model}_{seq_length}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b8a30",
   "metadata": {},
   "source": [
    "# Token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6ef1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
