{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebef44a",
   "metadata": {},
   "source": [
    "# Breaching privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756fc5f",
   "metadata": {},
   "source": [
    "This notebook does the same job as the cmd-line tool `simulate_breach.py`, but also directly visualizes the user data and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b850eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import breaching\n",
    "import logging, sys\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)], format='%(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5e214",
   "metadata": {},
   "source": [
    "### Initialize cfg object and system setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd663b",
   "metadata": {},
   "source": [
    "This will print out all configuration options. \n",
    "There are a lot of possible configurations, but there is usually no need to worry about most of these. Below, a few options are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26070d66",
   "metadata": {},
   "source": [
    "Choose `case/data=` `shakespeare`, `wikitext`over `stackoverflow` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7dc3a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating use case single_imagenet with server type honest_but_curious.\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize(config_path=\"config\"):\n",
    "    cfg = hydra.compose(config_name='cfg', overrides=['case/data=shakespeare', \n",
    "                                                      'case.model=transformer3',\n",
    "                                                    'case.server.has_external_data=True'])\n",
    "    print(f'Investigating use case {cfg.case.name} with server type {cfg.case.server.name}.')\n",
    "          \n",
    "device = torch.device(f'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.backends.cudnn.benchmark = cfg.case.impl.benchmark\n",
    "setup = dict(device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c5fb1",
   "metadata": {},
   "source": [
    "### Modify config options here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0764ef",
   "metadata": {},
   "source": [
    "You can use `.attribute` access to modify any of these configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac118ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.case.user.num_data_points = 5 # How many sentences?\n",
    "cfg.case.user.user_idx = 0 # From which user?\n",
    "cfg.case.data.shape = [30] # This is the sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f64389",
   "metadata": {},
   "source": [
    "### Instantiate all parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30235b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing user ALL_S_WELL_THAT_ENDS_WELL_CELIA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7633a79882b34c718d3b276ac08b76f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978174a203a24508871f7e1ad1380b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, loss_fn = breaching.cases.construct_model(cfg.case.model, cfg.case.data, pretrained=True)\n",
    "# Server:\n",
    "server = breaching.cases.construct_server(model, loss_fn, cfg.case, setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a6ef7",
   "metadata": {},
   "source": [
    "## Modify the model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3f415bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing feature distribution before the linear1 layer from external data.\n",
      "Feature mean is -0.053457196801900864, feature std is 0.6983630657196045.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from breaching.attacks.analytic_transformer_utils import * \n",
    "proportion = 8/96\n",
    "portion = int(proportion * model.encoder.embedding_dim)\n",
    "weights = torch.randn(model.encoder.embedding_dim)\n",
    "weights[:portion] = torch.zeros(portion)\n",
    "std, mu = torch.std_mean(weights)\n",
    "measurement = (weights - mu) / std / math.sqrt(model.encoder.embedding_dim) # Here's our linear measurement\n",
    "std, mean = feature_distribution(model, server, measurement)\n",
    "make_imprint_layer(model, measurement, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "063acf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30])\n",
      "torch.Size([5, 30, 96])\n",
      "Computing feature distribution before the linear1 layer from external data.\n",
      "Feature mean is -0.0047038705088198185, feature std is 0.025192659348249435.\n"
     ]
    }
   ],
   "source": [
    "set_MHA(model, server, sequence_token_weight=100, pos=0, attention_block=1, v_proportion=proportion)\n",
    "std, mean = feature_distribution(model, server, measurement, block_num=1)\n",
    "make_imprint_layer(model, measurement, mean, std, block_num=1, self_attn=True)\n",
    "model.transformer_encoder.layers[2].linear1.weight.data =\\\n",
    "    torch.zeros_like(model.transformer_encoder.layers[2].linear1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0fe7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize changes:\n",
    "model = server.vet_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d6666e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2cdc7e075e42f8bc856e5802bd8371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4e2a7f843942f58e832ba24c9b968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct the user here:\n",
    "user = breaching.cases.construct_user(model, loss_fn, cfg.case, setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c0ad6",
   "metadata": {},
   "source": [
    "### Simulate an attacked FL protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058bcc2",
   "metadata": {},
   "source": [
    "True user data is returned only for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f2a2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_payload = server.distribute_payload()\n",
    "shared_data, true_user_data = user.compute_local_updates(server_payload) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2867bcc",
   "metadata": {},
   "source": [
    "# Reconstruct user data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344527a",
   "metadata": {},
   "source": [
    "### Preliminary getting gradients, and recovering embeddings at different places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b951ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145, 96])\n",
      "torch.Size([145, 96])\n"
     ]
    }
   ],
   "source": [
    "grad_dict = dict([(k,v) for (k,_), (v) in zip(model.named_parameters(), shared_data['gradients'])])\n",
    "leaked_tokens = ((grad_dict['encoder.weight'] != 0).sum(dim=1) > 0).nonzero().squeeze().cpu() # Bag of words tokens\n",
    "\n",
    "weight_grad = grad_dict['transformer_encoder.layers.0.linear1.weight'].detach().clone().cpu()\n",
    "bias_grad = grad_dict['transformer_encoder.layers.0.linear1.bias'].detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "pos_recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed positionally encoded features\n",
    "no_pos_recs = model.transformer_encoder.layers[0].norm1((model.encoder(leaked_tokens) * math.sqrt(model.encoder.embedding_dim))).cpu()\n",
    "\n",
    "\n",
    "weight_grad = grad_dict['transformer_encoder.layers.1.linear1.weight'].detach().clone().cpu()\n",
    "bias_grad = grad_dict['transformer_encoder.layers.1.linear1.bias'].detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "attn_recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed post-mha embeddings\n",
    "print(pos_recs.shape)\n",
    "print(attn_recs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88bb68",
   "metadata": {},
   "source": [
    "### Group words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f325a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group words into sentence groups\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment # Better than greedy search? \n",
    "\n",
    "# Now, we need to map positionally encoded tokens to recovered \"attended\" embeddings\n",
    "pos_attn_coeffs = torch.zeros((len(pos_recs), len(attn_recs)))\n",
    "for i in range(len(pos_recs)):\n",
    "    for j in range(len(attn_recs)):\n",
    "        pos_attn_coeffs[i,j] = np.corrcoef(pos_recs[i][portion:].detach().numpy(), attn_recs[j][portion:].detach().numpy())[0,1]\n",
    "row_ind, col_ind = linear_sum_assignment(pos_attn_coeffs.numpy(), maximize=True)\n",
    "assignment_list = [(y,pos_recs[x]) for (x,y) in zip(row_ind, col_ind)]\n",
    "pos_lookup = dict(assignment_list)\n",
    "\n",
    "# Next, we need to map the attn embeddings to groups\n",
    "group_coeffs = torch.zeros((len(attn_recs), len(attn_recs)))\n",
    "for i in range(len(attn_recs)):\n",
    "    for j in range(len(attn_recs)):\n",
    "        group_coeffs[i,j] = np.corrcoef(attn_recs[i][:portion].detach().numpy(), attn_recs[j][:portion].detach().numpy())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87f69b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30\n",
      "1 29\n",
      "2 55\n",
      "57 1\n",
      "63 29\n",
      "129 1\n"
     ]
    }
   ],
   "source": [
    "group_dict = dict()\n",
    "num_groups = 0\n",
    "seen = set()\n",
    "for i in range(group_coeffs.shape[0]):\n",
    "    if i not in seen:\n",
    "        flag = group_coeffs[i].argmax()\n",
    "        # What threshhold to pick here? there should be a better way? \n",
    "        new_group = (group_coeffs[i] >= 0.95).nonzero().tolist() \n",
    "        print(i, len(new_group))\n",
    "        new_group = [x[0] for x in new_group]\n",
    "        if flag in group_dict:\n",
    "            group_num = group_coeffs[flag]\n",
    "        else:\n",
    "            group_num = num_groups\n",
    "            num_groups += 1\n",
    "        for x in new_group: \n",
    "            group_dict[x] = group_num\n",
    "            seen.add(x)\n",
    "\n",
    "from collections import defaultdict\n",
    "new_group_dict = defaultdict(list)\n",
    "for k, v in group_dict.items():\n",
    "    if k in pos_lookup:\n",
    "        new_group_dict[v].append(pos_lookup[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ee477c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yonder comes my master, your brother.But do not so. I have five hundred crowns,I scarce can speak to thank you for myself\n",
      ".\n",
      "[Coming forward] Sweet masters, be patient; for your father'sremembrance, be at accord.\n",
      "Is 'old dog'\n",
      " my reward? Most true, I have lost my teeth inCome not within these doors; within this roof\n",
      "The enemy of all your graces lives\n",
      ".\n",
      "Your brother- no, no brother; yet the son-\n",
      "Yet not the son; I will not call him son\n",
      "Of him I\n",
      " was about to call his father-\n",
      "Hath heard your praises; and this night he means\n",
      "To burn the lodging where you use to lie,\n"
     ]
    }
   ],
   "source": [
    "user.print(true_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18ec4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for group in new_group_dict.keys(): \n",
    "    sentences.append(recover_from_group(model, server, new_group_dict[group], no_pos_recs, leaked_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17c4ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally return a dict with keys data and labels\n",
    "reconstructed_user_data = dict(data=sentences, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33b7388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wasonder comes my master your.But do not soath I this five hundred crowns beI scarce can speak to thank you for myselfY have\n",
      " was about to call his father-\n",
      "Hath heard your praises; and he means theseTo burn the where you useold lie, nightComing\n",
      ".\n",
      "[But forward],TheH speak patient where for heard father's trueembranceathYet accord call teeth son 'Of him'YourComing- Sweet mastersonder yet Most your theserem the I will not gr praises dogaces no brother be; thank canIs\n",
      " at\n",
      " my? Most true, I have lost teeth inCome not within these doors;'roof\n",
      "The enemy of all your graces livesonder this\n",
      " reward\n"
     ]
    }
   ],
   "source": [
    "user.print(reconstructed_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdbf58",
   "metadata": {},
   "source": [
    "### Check metrics: (can't get to work for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29e98725",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (29) and references (30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_146918/229912699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m metrics = breaching.analysis.report(reconstructed_user_data, true_user_data, [server_payload], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                     server.model, cfg_case=cfg.case, setup=setup)\n",
      "\u001b[0;32m/cmlscratch/lfowl/breaching/breaching/analysis/analysis.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(reconstructed_user_data, true_user_data, server_payload, model, order_batch, compute_full_iip, compute_rpsnr, compute_ssim, cfg_case, setup)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver_payload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"modality\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmodality_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_text_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_user_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_user_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_payload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         modality_metrics = _run_vision_metrics(\n",
      "\u001b[0;32m/cmlscratch/lfowl/breaching/breaching/analysis/analysis.py\u001b[0m in \u001b[0;36m_run_text_metrics\u001b[0;34m(reconstructed_user_data, true_user_data, server_payload, cfg_case)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Accuracy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrec_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_user_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_user_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrec_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mtext_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/lfowl/miniconda3/envs/breaching/lib/python3.9/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0;34mf\"Input references: {summarize_if_long_list(references)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 )\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (29) and references (30)"
     ]
    }
   ],
   "source": [
    "metrics = breaching.analysis.report(reconstructed_user_data, true_user_data, [server_payload], \n",
    "                                    server.model, cfg_case=cfg.case, setup=setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74841e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
