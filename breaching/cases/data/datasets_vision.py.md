## **ğŸ“Œ `_build_dataset_vision()` ä»£ç è§£æ**

### **ğŸ”¹ ä½œç”¨**
è¯¥å‡½æ•°ç”¨äº **æ„é€ è®¡ç®—æœºè§†è§‰ï¼ˆvisionï¼‰æ•°æ®é›†**ï¼Œå¹¶è¿”å›ï¼š
1. **å·²åŠ è½½çš„ `dataset`**
2. **æ•°æ®æ‰¹å¤„ç†å‡½æ•° `collate_fn`**ï¼ˆç”¨äº `DataLoader`ï¼‰

---

## **ğŸ“Œ ä»£ç ç»“æ„**
```python
def _build_dataset_vision(cfg_data, split, can_download=True):
```
- **è¾“å…¥å‚æ•°**
  - `cfg_data`ï¼šåŒ…å«æ•°æ®é›†åç§°ã€è·¯å¾„ã€æ˜¯å¦æ ‡å‡†åŒ–ç­‰é…ç½®ä¿¡æ¯ã€‚
  - `split`ï¼šæ•°æ®é›†åˆ’åˆ†ï¼ˆå¦‚ `"training"`ã€`"validation"`ï¼‰ã€‚
  - `can_download`ï¼šæ˜¯å¦å…è®¸è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†ã€‚

- **è¿”å›å€¼**
  - `dataset`ï¼šåŠ è½½çš„æ•°æ®é›†ï¼ˆ`torchvision.datasets`ï¼‰ã€‚
  - `collate_fn`ï¼šæ•°æ®æ‰¹å¤„ç†å‡½æ•°ã€‚

---

## **1. è®¾ç½®é»˜è®¤çš„ `ToTensor()` è½¬æ¢**
```python
_default_t = torchvision.transforms.ToTensor()
```
- è¯¥è½¬æ¢ **å°† PIL å›¾ç‰‡è½¬æ¢ä¸º `torch.Tensor`**ã€‚
- **å¦‚æœåç»­æœªæŒ‡å®šé¢å¤–çš„å›¾åƒå¢å¼ºè½¬æ¢ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ `ToTensor()`ã€‚**

---

## **2. å¤„ç† `cfg_data.path`**
```python
cfg_data.path = os.path.expanduser(cfg_data.path)
```
- **å±•å¼€ `~` ä¸ºç”¨æˆ·ç›®å½•è·¯å¾„**ï¼Œä¿è¯è·¯å¾„æ­£ç¡®ã€‚

---

## **3. é€‰æ‹©ä¸åŒçš„æ•°æ®é›†**
### **ğŸ“Œ 3.1 å¤„ç† `CIFAR10`**
```python
if cfg_data.name == "CIFAR10":
    dataset = torchvision.datasets.CIFAR10(
        root=cfg_data.path, train=split == "training", download=can_download, transform=_default_t,
    )
    dataset.lookup = dict(zip(list(range(len(dataset))), dataset.targets))
```
- **ä½¿ç”¨ `torchvision.datasets.CIFAR10` ç›´æ¥åŠ è½½ CIFAR-10**ã€‚
- `train=split == "training"`ï¼š
  - **å¦‚æœ `split="training"`**ï¼Œåˆ™åŠ è½½è®­ç»ƒé›†ï¼›
  - å¦åˆ™åŠ è½½æµ‹è¯•é›†ï¼ˆ`train=False`ï¼‰ã€‚
- **æ•°æ® `lookup` æœºåˆ¶**
  ```python
  dataset.lookup = dict(zip(list(range(len(dataset))), dataset.targets))
  ```
  - **æ„é€ ç´¢å¼•åˆ°ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„**ï¼Œæ–¹ä¾¿åç»­å¤„ç†ã€‚

---

### **ğŸ“Œ 3.2 å¤„ç† `CIFAR100`**
```python
elif cfg_data.name == "CIFAR100":
    dataset = torchvision.datasets.CIFAR100(
        root=cfg_data.path, train=split == "training", download=can_download, transform=_default_t,
    )
    dataset.lookup = dict(zip(list(range(len(dataset))), dataset.targets))
```
- é€»è¾‘ä¸ `CIFAR10` **å®Œå…¨ç›¸åŒ**ï¼Œåªæ˜¯æ•°æ®é›†æ¢æˆ `CIFAR100`ã€‚

---

### **ğŸ“Œ 3.3 å¤„ç† `ImageNet`**
```python
elif cfg_data.name == "ImageNet":
    dataset = torchvision.datasets.ImageNet(
        root=cfg_data.path, split="train" if "train" in split else "val", transform=_default_t,
    )
    dataset.lookup = dict(zip(list(range(len(dataset))), [label for (_, label) in dataset.samples]))
```
- `split="train"` æˆ– `"val"` ç¡®å®šåŠ è½½è®­ç»ƒé›†æˆ–éªŒè¯é›†ã€‚
- **`dataset.samples` æ˜¯ `(å›¾ç‰‡è·¯å¾„, ç±»åˆ«æ ‡ç­¾)` çš„åˆ—è¡¨**ã€‚
- `lookup` è®°å½• **ç´¢å¼•åˆ°ç±»åˆ«çš„æ˜ å°„**ï¼š
  ```python
  dataset.lookup = dict(zip(list(range(len(dataset))), [label for (_, label) in dataset.samples]))
  ```

---

### **ğŸ“Œ 3.4 å¤„ç† `ImageNetAnimals`**
```python
elif cfg_data.name == "ImageNetAnimals":
    dataset = torchvision.datasets.ImageNet(
        root=cfg_data.path, split="train" if "train" in split else "val", transform=_default_t,
    )
    dataset.lookup = dict(zip(list(range(len(dataset))), [label for (_, label) in dataset.samples]))
    indices = [idx for (idx, label) in dataset.lookup.items() if label < 397]
    dataset.classes = dataset.classes[:397]
    dataset.samples = [dataset.samples[i] for i in indices]
    dataset.lookup = dict(zip(list(range(len(dataset))), [label for (_, label) in dataset.samples]))
```
- **åŸºäº ImageNet è¿‡æ»¤ `label < 397` çš„ç±»åˆ«**ï¼Œç”¨äº **å­é›†å®éªŒ**ï¼ˆä»…ä¿ç•™å‰ 397 ç±»ï¼‰ã€‚
- **è°ƒæ•´ `dataset.classes` å’Œ `dataset.samples`**ï¼Œä½¿å…¶ä¸ `lookup` ä¸€è‡´ã€‚

---

### **ğŸ“Œ 3.5 å¤„ç† `TinyImageNet`**
```python
elif cfg_data.name == "TinyImageNet":
    dataset = TinyImageNet(
        root=cfg_data.path, split=split, download=can_download, transform=_default_t, cached=True,
    )
    dataset.lookup = dict(zip(list(range(len(dataset))), dataset.targets))
```
- **TinyImageNet** æ˜¯ä¸€ä¸ªæ›´å°çš„ ImageNet å˜ä½“ï¼ˆ200 ç±»ï¼Œæ¯ç±» 500 å¼ å›¾ï¼‰ã€‚
- `cached=True` å¯èƒ½ **å¯ç”¨ç¼“å­˜ï¼Œæé«˜åŠ è½½æ•ˆç‡**ã€‚

---

### **ğŸ“Œ 3.6 å¤„ç† `Birdsnap`**
```python
elif cfg_data.name == "Birdsnap":
    dataset = Birdsnap(root=cfg_data.path, split=split, download=can_download, transform=_default_t)
    dataset.lookup = dict(zip(list(range(len(dataset))), dataset.labels))
```
- **Birdsnap æ˜¯é¸Ÿç±»åˆ†ç±»æ•°æ®é›†**ï¼Œ`labels` å­˜å‚¨ç±»åˆ«ç´¢å¼•ã€‚

---

### **ğŸ“Œ 3.7 æ•°æ®é›†ä¸å­˜åœ¨æ—¶æŠ›å‡ºé”™è¯¯**
```python
else:
    raise ValueError(f"Invalid dataset {cfg_data.name} provided.")
```
- å¦‚æœ `cfg_data.name` ä¸æ˜¯æ”¯æŒçš„æ•°æ®é›†ï¼Œåˆ™æŠ›å‡º `ValueError`ã€‚

---

## **4. è®¡ç®—æ•°æ®å‡å€¼å’Œæ ‡å‡†å·®**
```python
if cfg_data.mean is None and cfg_data.normalize:
    data_mean, data_std = _get_meanstd(dataset)
    cfg_data.mean = data_mean
    cfg_data.std = data_std
```
- **å¦‚æœ `cfg_data.mean` æœªå®šä¹‰ä¸”éœ€è¦æ ‡å‡†åŒ–ï¼Œåˆ™è‡ªåŠ¨è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®**ã€‚
- `mean` å’Œ `std` å¯ç”¨äºåç»­ **å›¾åƒæ ‡å‡†åŒ–é¢„å¤„ç†**ã€‚

---

## **5. è§£ææ•°æ®å¢å¼º**
```python
transforms = _parse_data_augmentations(cfg_data, split)
```
- **è§£ææ•°æ®å¢å¼º**ï¼ˆå¦‚ `RandomCrop`ã€`HorizontalFlip`ï¼‰ã€‚
- **è‹¥ `cfg_data.augmentations` é…ç½®äº†æ•°æ®å¢å¼ºï¼Œåˆ™è¿”å›å¯¹åº”çš„ `transforms`**ã€‚

---

## **6. åº”ç”¨æ•°æ®å˜æ¢**
```python
dataset.transform = transforms if transforms is not None else None
```
- **å¦‚æœ `transforms` å­˜åœ¨ï¼Œåˆ™åº”ç”¨åˆ° `dataset.transform`**ã€‚
- **å¦åˆ™ï¼Œé»˜è®¤ä½¿ç”¨ `ToTensor()`**ã€‚

---

## **7. å­˜å‚¨å‡å€¼å’Œæ ‡å‡†å·®**
```python
if cfg_data.normalize:
    dataset.mean = cfg_data.mean
    dataset.std = cfg_data.std
else:
    dataset.mean = [0]
    dataset.std = [1]
```
- **å¦‚æœ `normalize=True`ï¼Œå­˜å‚¨ `mean` å’Œ `std`**ã€‚
- **å¦åˆ™ï¼Œè®¾ç½® `mean=0`ï¼Œ`std=1`ï¼ˆæ— æ ‡å‡†åŒ–ï¼‰ã€‚**

---

## **8. å¤„ç†æ•°æ®å­é›†**
```python
if cfg_data.size < len(dataset):
    dataset = Subset(dataset, torch.arange(0, cfg_data.size))
```
- **å¦‚æœ `cfg_data.size` å°äºæ•°æ®é›†æ€»å¤§å°ï¼Œåˆ™** **è£å‰ªæ•°æ®é›†**ï¼ˆç”¨äºå°è§„æ¨¡å®éªŒï¼‰ã€‚

---

## **9. è¿”å›æ•°æ®é›†å’Œ `collate_fn`**
```python
collate_fn = _torchvision_collate
return dataset, collate_fn
```
- **`collate_fn` å¤„ç†æ•°æ®æ‰¹æ¬¡ï¼ˆé»˜è®¤ `torch.utils.data.default_collate`ï¼‰ã€‚**
- **è¿”å› `dataset` å’Œ `collate_fn`**ã€‚

---

## **ğŸ“Œ ä»£ç æµç¨‹æ€»ç»“**
1. **åˆå§‹åŒ– `ToTensor()` ä½œä¸ºé»˜è®¤å˜æ¢**ã€‚
2. **åŠ è½½ä¸åŒæ•°æ®é›†**ï¼ˆ`CIFAR10`ã€`ImageNet` ç­‰ï¼‰ã€‚
3. **è®¡ç®—æ•°æ®å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆå¦‚æœ `normalize=True`ï¼‰**ã€‚
4. **è§£ææ•°æ®å¢å¼º**ï¼ˆå¦‚ `RandomCrop`ï¼‰ã€‚
5. **åº”ç”¨å˜æ¢ `dataset.transform`**ã€‚
6. **å­˜å‚¨ `mean` å’Œ `std`**ã€‚
7. **å¦‚æœ `cfg_data.size` å°äºæ•°æ®æ€»é‡ï¼Œåˆ™è£å‰ªæ•°æ®**ã€‚
8. **è¿”å› `dataset` å’Œ `collate_fn`**ã€‚

---

## **ğŸ“Œ ç»“è®º**
âœ… **æ”¯æŒ `CIFAR10`ã€`ImageNet` ç­‰å¤šä¸ªæ•°æ®é›†**ã€‚  
âœ… **å¯è‡ªåŠ¨è®¡ç®— `mean/std`ï¼Œå¹¶æ”¯æŒæ ‡å‡†åŒ–**ã€‚  
âœ… **æ”¯æŒæ•°æ®å¢å¼ºã€å­é›†é€‰æ‹©ã€ä¸‹è½½ç­‰åŠŸèƒ½**ã€‚  

ğŸ’¡ **æ€»ç»“ï¼šè¯¥å‡½æ•°æ˜¯è®¡ç®—æœºè§†è§‰æ•°æ®åŠ è½½çš„æ ¸å¿ƒç»„ä»¶ï¼Œä¸ºè”é‚¦å­¦ä¹ å’Œéšç§æ”»å‡»ä»»åŠ¡æä¾›æ•°æ®æ”¯æŒï¼ğŸš€**



## **ğŸ“Œ `_torchvision_collate()` ä»£ç è§£æ**

### **ğŸ”¹ ä½œç”¨**
- è¯¥å‡½æ•°æ˜¯ä¸€ä¸ª **è‡ªå®šä¹‰çš„æ•°æ®æ‰¹æ¬¡åˆå¹¶å‡½æ•°ï¼ˆcollate functionï¼‰**ï¼Œç”¨äº `torch.utils.data.DataLoader`ã€‚
- **ä¿®æ”¹é»˜è®¤ `collate_fn` çš„è¡Œä¸º**ï¼Œä½¿å…¶è¿”å› **å­—å…¸æ ¼å¼**ï¼š
  ```python
  return dict(inputs=..., labels=...)
  ```
- **ç¡®ä¿å¼ é‡åœ¨å¤šçº¿ç¨‹æ•°æ®åŠ è½½æ—¶ä½¿ç”¨å…±äº«å†…å­˜ï¼Œé¿å…é¢å¤–çš„æ‹·è´ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡**ã€‚

---

## **ğŸ“Œ ä»£ç ç»“æ„**
```python
def _torchvision_collate(batch):
```
- **è¾“å…¥å‚æ•°**
  - `batch`ï¼šä¸€ä¸ª **æ‰¹æ¬¡æ ·æœ¬åˆ—è¡¨**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    ```python
    batch = [(img1, label1), (img2, label2), (img3, label3), ...]
    ```
    å…¶ä¸­ï¼š
    - `img_i` æ˜¯ä¸€ä¸ª `torch.Tensor`ï¼ˆå›¾åƒæ•°æ®ï¼‰
    - `label_i` æ˜¯ä¸€ä¸ªæ•´æ•°ï¼ˆç±»åˆ«æ ‡ç­¾ï¼‰

- **è¿”å›å€¼**
  - **`dict(inputs=å¼ é‡, labels=å¼ é‡)`**
  - **æ ¼å¼ç¤ºä¾‹**ï¼š
    ```python
    {
        "inputs": torch.Size([batch_size, C, H, W]),  # å›¾åƒæ•°æ®
        "labels": torch.Size([batch_size])  # ç›®æ ‡æ ‡ç­¾
    }
    ```

---

## **1. `transposed = list(zip(*batch))`ï¼šå°† batch è½¬ç½®**
```python
transposed = list(zip(*batch))
```
- **ä½œç”¨**ï¼šå°† `batch` **æ‹†åˆ†æˆä¸¤ä¸ªåˆ—è¡¨**ï¼š
  - `transposed[0]`ï¼šå›¾åƒ `img_i` åˆ—è¡¨
  - `transposed[1]`ï¼šæ ‡ç­¾ `label_i` åˆ—è¡¨

ğŸ”¹ **ç¤ºä¾‹**
```python
batch = [(img1, label1), (img2, label2), (img3, label3)]
transposed = list(zip(*batch))
```
**ç»“æœ**
```python
transposed[0] = (img1, img2, img3)  # å›¾åƒæ•°æ®
transposed[1] = (label1, label2, label3)  # æ ‡ç­¾
```

---

## **2. `_stack_tensor(tensor_list)`ï¼šåˆå¹¶ `inputs` å¼ é‡**
```python
def _stack_tensor(tensor_list):
```
- **ä½œç”¨**ï¼šå°† `tensor_list` **å †å ** æˆä¸€ä¸ª `torch.Tensor`ï¼Œå¹¶ **åœ¨å¤šçº¿ç¨‹æ¨¡å¼ä¸‹ä½¿ç”¨å…±äº«å†…å­˜**ã€‚

### **ğŸ“Œ ä»£ç è§£æ**
```python
elem = tensor_list[0]
elem_type = type(elem)
out = None
```
- è·å– `tensor_list` ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæ£€æŸ¥å…¶ç±»å‹ï¼ˆåº”ä¸º `torch.Tensor`ï¼‰ã€‚

```python
if torch.utils.data.get_worker_info() is not None:
```
- **æ£€æŸ¥å½“å‰æ˜¯å¦åœ¨ `DataLoader` çš„å­è¿›ç¨‹ä¸­**ï¼ˆå³å¤šçº¿ç¨‹æ•°æ®åŠ è½½ï¼‰ã€‚
- **å¦‚æœåœ¨å­è¿›ç¨‹ä¸­ï¼Œåˆ†é…å…±äº«å†…å­˜ï¼Œæé«˜æ•ˆç‡**ï¼š
  ```python
  numel = sum(x.numel() for x in tensor_list)
  storage = elem.storage()._new_shared(numel)
  out = elem.new(storage)
  ```
  - **è®¡ç®— `tensor_list` ä¸­æ‰€æœ‰å¼ é‡çš„å…ƒç´ æ•°é‡ `numel`**ã€‚
  - **åˆ›å»ºå…±äº«å†…å­˜ `storage`**ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®æ‹·è´ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡ã€‚

```python
return torch.stack(tensor_list, 0, out=out)
```
- **æœ€ç»ˆä½¿ç”¨ `torch.stack()` åˆå¹¶å¼ é‡**ï¼Œå¹¶è¿”å›å…±äº«å†…å­˜ä¸­çš„ `out`ã€‚

---

## **3. `labels = torch.tensor(transposed[1])`**
```python
return dict(inputs=_stack_tensor(transposed[0]), labels=torch.tensor(transposed[1]))
```
- **å°† `labels` åˆ—è¡¨è½¬æ¢ä¸º `torch.Tensor`**ï¼š
  ```python
  labels = torch.tensor(transposed[1])
  ```

- **æœ€ç»ˆè¿”å›**
  ```python
  return {
      "inputs": batch_size å¼ é‡,
      "labels": batch_size å¼ é‡
  }
  ```

---

## **ğŸ“Œ ä»£ç æµç¨‹æ€»ç»“**
1. **å°† `batch` è½¬ç½®**ï¼š
   ```python
   transposed = list(zip(*batch))
   ```
   - `transposed[0]`ï¼šæ‰€æœ‰å›¾åƒ
   - `transposed[1]`ï¼šæ‰€æœ‰æ ‡ç­¾

2. **åˆå¹¶ `inputs`**
   ```python
   inputs = _stack_tensor(transposed[0])
   ```
   - **å¤šçº¿ç¨‹æ¨¡å¼ä¸‹** ä½¿ç”¨ **å…±äº«å†…å­˜** åŠ é€Ÿæ•°æ®åŠ è½½ã€‚

3. **åˆå¹¶ `labels`**
   ```python
   labels = torch.tensor(transposed[1])
   ```

4. **è¿”å›å­—å…¸**
   ```python
   return dict(inputs=inputs, labels=labels)
   ```

---

## **ğŸ“Œ ç¤ºä¾‹**
### **ğŸ¯ è¾“å…¥**
```python
batch = [
    (torch.randn(3, 32, 32), 0),  # å›¾åƒ & æ ‡ç­¾
    (torch.randn(3, 32, 32), 1),
    (torch.randn(3, 32, 32), 2)
]
```

### **ğŸ¯ ä»£ç æ‰§è¡Œ**
```python
result = _torchvision_collate(batch)
```

### **ğŸ¯ è¾“å‡º**
```python
{
    "inputs": torch.Size([3, 3, 32, 32]),  # 3 å¼ å›¾ç‰‡ï¼Œ3 é€šé“ï¼Œ32x32 å¤§å°
    "labels": torch.Size([3])  # 3 ä¸ªæ ‡ç­¾
}
```

---

## **ğŸ“Œ ç»“è®º**
âœ… **è¯¥å‡½æ•°è‡ªå®šä¹‰ `collate_fn`ï¼Œè¿”å›å­—å…¸æ ¼å¼ `{inputs, labels}`**  
âœ… **ä½¿ç”¨ `torch.stack()` ç»„åˆ `inputs`ï¼Œæé«˜ GPU è®­ç»ƒæ•ˆç‡**  
âœ… **æ”¯æŒå¤šçº¿ç¨‹æ•°æ®åŠ è½½ï¼Œåœ¨å­è¿›ç¨‹ä¸­ä½¿ç”¨å…±äº«å†…å­˜ï¼Œé¿å…æ‹·è´**  

ğŸ’¡ **æ€»ç»“ï¼š`_torchvision_collate()` æ˜¯ä¸€ä¸ªä¼˜åŒ–ç‰ˆçš„ `collate_fn`ï¼Œç‰¹åˆ«é€‚ç”¨äº `DataLoader` è¿›è¡Œé«˜æ•ˆæ•°æ®åŠ è½½ï¼ğŸš€**

---
---
---

## **ğŸ“Œ `_split_dataset_vision()` ä»£ç è§£æ**

### **ğŸ”¹ ä½œç”¨**
- **å¯¹è§†è§‰æ•°æ®é›† `dataset` è¿›è¡Œç”¨æˆ·çº§åˆ«åˆ’åˆ†ï¼ˆPartitioningï¼‰**ï¼Œé€‚ç”¨äº **è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰** åœºæ™¯ã€‚
- **ä¸åŒçš„ `partition` æ–¹æ¡ˆå†³å®šäº†æ•°æ®å¦‚ä½•åˆ†é…ç»™ç”¨æˆ·**ã€‚
- **æ”¯æŒä¸åŒçš„æ•°æ®åˆ’åˆ†ç­–ç•¥**ï¼š
  - `balanced`ï¼ˆå‡è¡¡åˆ’åˆ†ï¼‰
  - `unique-class`ï¼ˆå”¯ä¸€ç±»åˆ«ï¼‰
  - `mixup`ï¼ˆæ··åˆåˆ’åˆ†ï¼‰
  - `feat_est`ï¼ˆç‰¹å®šç±»åˆ«åˆ’åˆ†ï¼‰
  - `random-full`ï¼ˆéšæœºä½†å…è®¸é‡å¤ï¼‰
  - `random`ï¼ˆéšæœºä½†ä¸é‡å¤ï¼‰
  - `none`ï¼ˆæ‰€æœ‰ç”¨æˆ·ä½¿ç”¨ç›¸åŒæ•°æ®ï¼‰

---

## **ğŸ“Œ ä»£ç ç»“æ„**
```python
def _split_dataset_vision(dataset, cfg_data, user_idx=None, return_full_dataset=False):
```
- **è¾“å…¥å‚æ•°**
  - `dataset`ï¼šå®Œæ•´æ•°æ®é›†ï¼ˆ`torchvision.datasets`ï¼‰ã€‚
  - `cfg_data`ï¼šæ•°æ®é›†çš„é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«åˆ’åˆ†æ–¹å¼ `partition`ã€‚
  - `user_idx`ï¼šå½“å‰ç”¨æˆ·ç´¢å¼•ï¼ˆæŒ‡å®šè¯¥ç”¨æˆ·åº”è¯¥ä½¿ç”¨çš„æ•°æ®ï¼‰ã€‚
  - `return_full_dataset`ï¼šæ˜¯å¦è¿”å›å®Œæ•´æ•°æ®é›†ï¼ˆå¦‚ç”¨äºæ•´ä½“åˆ†æï¼‰ã€‚

- **è¿”å›å€¼**
  - `dataset`ï¼šåˆ’åˆ†åçš„å­æ•°æ®é›†ã€‚

---

## **1. å¤„ç† `return_full_dataset=True` çš„æƒ…å†µ**
```python
if not return_full_dataset:
```
- **å¦‚æœ `return_full_dataset=True`ï¼Œåˆ™ç›´æ¥è¿”å›å®Œæ•´æ•°æ®é›†**ï¼Œä¸è¿›è¡Œåˆ’åˆ†ã€‚

---

## **2. é€‰æ‹© `user_idx`**
```python
if user_idx is None:
    user_idx = torch.randint(0, cfg_data.default_clients, (1,))
else:
    if user_idx > cfg_data.default_clients:
        raise ValueError("This user index exceeds the maximal number of clients.")
```
- **å¦‚æœ `user_idx` ä¸ºç©º**ï¼Œåˆ™éšæœºåˆ†é…ä¸€ä¸ªç”¨æˆ·ç´¢å¼•ã€‚
- **å¦‚æœ `user_idx` è¶…å‡º `cfg_data.default_clients`ï¼ˆæ€»ç”¨æˆ·æ•°ï¼‰**ï¼ŒæŠ›å‡ºé”™è¯¯ã€‚

---

## **3. æ•°æ®åˆ’åˆ†ç­–ç•¥**
æ ¹æ® **`cfg_data.partition`** é€‰æ‹©ä¸åŒçš„ **æ•°æ®åˆ’åˆ†æ–¹å¼**ã€‚

---

### **ğŸ“Œ 3.1 `balanced`ï¼ˆå‡è¡¡åˆ’åˆ†ï¼‰**
```python
if cfg_data.partition == "balanced":
    data_per_class_per_user = len(dataset) // len(dataset.classes) // cfg_data.default_clients
    if data_per_class_per_user < 1:
        raise ValueError("Too many clients for a balanced dataset.")
    data_ids = []
    for class_idx, _ in enumerate(dataset.classes):
        data_with_class = [idx for (idx, label) in dataset.lookup.items() if label == class_idx]
        data_ids += data_with_class[
            user_idx * data_per_class_per_user : data_per_class_per_user * (user_idx + 1)
        ]
    dataset = Subset(dataset, data_ids)
```
- **æ¯ä¸ªç”¨æˆ·è·å¾—** **ç›¸åŒæ•°é‡çš„æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬**ï¼ˆå‡è¡¡åˆ’åˆ†ï¼‰ã€‚
- è®¡ç®— **æ¯ä¸ªç”¨æˆ·æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°**ï¼š
  ```python
  data_per_class_per_user = len(dataset) // len(dataset.classes) // cfg_data.default_clients
  ```
- **æ•°æ®é€‰æ‹©æ–¹å¼**
  ```python
  for class_idx in dataset.classes:
      data_with_class = [idx for (idx, label) in dataset.lookup.items() if label == class_idx]
      data_ids += data_with_class[user_idx * data_per_class_per_user : (user_idx + 1) * data_per_class_per_user]
  ```
  - **éå†æ‰€æœ‰ç±»åˆ«**ï¼Œä¸º `user_idx` é€‰æ‹©å±äºè¯¥ç±»åˆ«çš„ `data_per_class_per_user` å¼ å›¾ç‰‡ã€‚

---

### **ğŸ“Œ 3.2 `unique-class`ï¼ˆå”¯ä¸€ç±»åˆ«ï¼‰**
```python
elif cfg_data.partition == "unique-class":
    data_ids = [idx for (idx, label) in dataset.lookup.items() if label == user_idx]
    dataset = Subset(dataset, data_ids)
```
- **æ¯ä¸ªç”¨æˆ·åªè·å¾—å•ä¸ªç±»åˆ«çš„æ•°æ®**ï¼ˆç±»åˆ« ID = ç”¨æˆ· IDï¼‰ã€‚
- **é€‚ç”¨äºâ€œæ¯ä¸ªç”¨æˆ·åªå­¦ä¸€ä¸ªç±»åˆ«â€çš„ FL ä»»åŠ¡**ã€‚

---

### **ğŸ“Œ 3.3 `mixup`ï¼ˆæ•°æ®æ··åˆåˆ’åˆ†ï¼‰**
```python
elif cfg_data.partition == "mixup":
    if "mixup_freq" in cfg_data:
        mixup_freq = cfg_data.mixup_freq
    else:
        mixup_freq = 2
    data_per_user = len(dataset) // cfg_data.default_clients
    last_id = len(dataset) - 1
    data_ids = []
    for i in range(data_per_user):
        data_ids.append(user_idx * data_per_user + i)
        data_ids.append(last_id - user_idx * data_per_user - i)
    dataset = Subset(dataset, data_ids)
```
- **æ•°æ®è¢«åŒå‘åˆ†é…**ï¼š
  - ä¸€éƒ¨åˆ†ä» `user_idx * data_per_user` å¼€å§‹
  - å¦ä¸€éƒ¨åˆ†ä» `last_id - user_idx * data_per_user` å€’åºå–
- **ç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„æ•°æ®åŒæ—¶æ¥è‡ªå¤´éƒ¨å’Œå°¾éƒ¨ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§**ã€‚

---

### **ğŸ“Œ 3.4 `feat_est`ï¼ˆç‰¹å®šç±»åˆ«åˆ’åˆ†ï¼‰**
```python
elif cfg_data.partition == "feat_est":
    num_data_points = cfg_data.num_data_points if "num_data_points" in cfg_data else 1
    target_label = cfg_data.target_label if "target_label" in cfg_data else 0
    data_ids = [idx for (idx, label) in dataset.lookup.items() if label == target_label]
    data_ids = data_ids[user_idx * num_data_points : (user_idx + 1) * num_data_points]
    dataset = Subset(dataset, data_ids)
```
- **ä»…ä¸º `target_label` æŒ‡å®šçš„ç±»åˆ«åˆ†é…æ•°æ®**ã€‚
- **é€‚ç”¨äºç‰¹å®šç±»åˆ«æ”»å‡»ï¼ˆFeature Estimationï¼‰**ã€‚

---

### **ğŸ“Œ 3.5 `random-full`ï¼ˆéšæœºä½†å…è®¸é‡å¤ï¼‰**
```python
elif cfg_data.partition == "random-full":
    data_per_user = len(dataset) // cfg_data.default_clients
    data_ids = torch.randperm(len(dataset))[:data_per_user]
    dataset = Subset(dataset, data_ids)
```
- **éšæœºé€‰æ‹©æ•°æ®ï¼Œå…è®¸ä¸åŒç”¨æˆ·å…±äº«æ•°æ®**ã€‚

---

### **ğŸ“Œ 3.6 `random`ï¼ˆéšæœºä½†ä¸é‡å¤ï¼‰**
```python
elif cfg_data.partition == "random":
    data_per_user = len(dataset) // cfg_data.default_clients
    generator = torch.Generator()
    generator.manual_seed(233)
    data_ids = torch.randperm(len(dataset))[user_idx * data_per_user : data_per_user * (user_idx + 1)]
    dataset = Subset(dataset, data_ids)
```
- **éšæœºåˆ’åˆ†æ•°æ®ï¼Œæ¯ä¸ªç”¨æˆ·æ•°æ®ä¸é‡å¤**ï¼ˆ**ä¿è¯ä¸åŒç”¨æˆ·æ•°æ®å”¯ä¸€æ€§**ï¼‰ã€‚
- **ä½¿ç”¨å›ºå®šéšæœºç§å­ `233` ä»¥ä¿è¯å¯å¤ç°**ã€‚

---

### **ğŸ“Œ 3.7 `none`ï¼ˆæ‰€æœ‰ç”¨æˆ·å…±äº«å®Œæ•´æ•°æ®é›†ï¼‰**
```python
elif cfg_data.partition == "none":
    pass
```
- **æ‰€æœ‰ç”¨æˆ·å…±äº«å®Œæ•´æ•°æ®é›†**ï¼ˆå³æ— åˆ’åˆ†ï¼‰ã€‚
- **ç”¨äº sanity checkï¼ˆå®Œæ•´æ€§æµ‹è¯•ï¼‰**ã€‚

---

### **ğŸ“Œ 3.8 æŠ›å‡ºæœªå®ç°çš„åˆ’åˆ†ç­–ç•¥**
```python
else:
    raise ValueError(f"Partition scheme {cfg_data.partition} not implemented.")
```
- **å¦‚æœ `partition` æœªåœ¨ä¸Šè¿°ç­–ç•¥ä¸­å®šä¹‰ï¼Œåˆ™æŠ›å‡ºé”™è¯¯**ã€‚

---

## **ğŸ“Œ ä»£ç æµç¨‹æ€»ç»“**
1. **æ£€æŸ¥æ˜¯å¦è¿”å›å®Œæ•´æ•°æ®é›†**
   - `return_full_dataset=True` â†’ ç›´æ¥è¿”å›å®Œæ•´æ•°æ®é›†ã€‚

2. **ç¡®å®šç”¨æˆ·ç´¢å¼•**
   - è‹¥ `user_idx=None`ï¼Œåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªã€‚

3. **é€‰æ‹©æ•°æ®åˆ’åˆ†ç­–ç•¥**
   - `balanced`ï¼šæ‰€æœ‰ç”¨æˆ·è·å¾—ç›¸åŒæ¯”ä¾‹çš„ç±»åˆ«æ•°æ®
   - `unique-class`ï¼šæ¯ä¸ªç”¨æˆ·åªè·å¾—å•ä¸ªç±»åˆ«
   - `mixup`ï¼šæ•°æ®æ··åˆï¼Œæå‡å¤šæ ·æ€§
   - `feat_est`ï¼šé’ˆå¯¹ç‰¹å®šç±»åˆ«è¿›è¡Œç‰¹å¾ä¼°è®¡
   - `random-full`ï¼šéšæœºæ•°æ®ï¼Œå¯èƒ½é‡å¤
   - `random`ï¼šéšæœºæ•°æ®ï¼Œä¸é‡å¤
   - `none`ï¼šæ‰€æœ‰ç”¨æˆ·å…±äº«å®Œæ•´æ•°æ®é›†

---

## **ğŸ“Œ ç»“è®º**
âœ… **æ”¯æŒå¤šç§æ•°æ®åˆ’åˆ†ç­–ç•¥ï¼Œé€‚ç”¨äºè”é‚¦å­¦ä¹ **  
âœ… **ä¿è¯ç”¨æˆ·æ•°æ®ç‹¬ç«‹æ€§ï¼Œæ”¯æŒå‡è¡¡ã€éšæœºã€å”¯ä¸€ç±»åˆ«ç­‰åˆ’åˆ†æ–¹å¼**  
âœ… **å¯å¤ç°ï¼ˆå›ºå®šéšæœºç§å­ï¼‰ï¼Œæ”¯æŒ FL è®­ç»ƒ**  

ğŸ’¡ **æ€»ç»“ï¼šè¯¥å‡½æ•°æ˜¯è”é‚¦å­¦ä¹ ä¸­â€œæ•°æ®åˆ’åˆ†â€çš„æ ¸å¿ƒï¼Œç¡®ä¿æ¯ä¸ªç”¨æˆ·è·å–æ­£ç¡®çš„æ•°æ®é›†ï¼ğŸš€**