## **ğŸ“Œ `construct_user()` å‡½æ•°è§£æ**

### **ğŸ”¹ ä½œç”¨**
- `construct_user()` æ˜¯ä¸€ä¸ª **ç”¨æˆ·å®ä¾‹åŒ–æ¥å£**ï¼Œæ ¹æ®ä¸åŒçš„ **ç”¨æˆ·ç±»å‹ï¼ˆ`user_type`ï¼‰** åˆ›å»ºä¸åŒçš„ç”¨æˆ·å¯¹è±¡ï¼š
  - **`local_gradient`** â†’ `UserSingleStep`ï¼ˆå•æ­¥æ¢¯åº¦è®¡ç®—ï¼‰
  - **`local_update`** â†’ `UserMultiStep`ï¼ˆæœ¬åœ°å¤šæ­¥æ›´æ–°ï¼‰
  - **`multiuser_aggregate`** â†’ `MultiUserAggregate`ï¼ˆå¤šç”¨æˆ·èšåˆï¼‰

- **è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰** ä¸­ï¼Œç”¨æˆ·ï¼ˆclientï¼‰ä¼š **è·å–æœåŠ¡å™¨æä¾›çš„æ¨¡å‹ï¼Œåœ¨æœ¬åœ°æ•°æ®ä¸Šè¿›è¡Œè®¡ç®—ï¼Œå¹¶è¿”å›æ›´æ–°ä¿¡æ¯**ã€‚
- `construct_user()` æ ¹æ® **é…ç½® `cfg_case.user.user_type`**ï¼Œå†³å®šåˆ›å»ºå“ªç§ç”¨æˆ·å¯¹è±¡ï¼Œå¹¶åŠ è½½ç›¸åº”çš„æ•°æ®ã€‚

---

## **ğŸ“Œ ä»£ç ç»“æ„**
```python
def construct_user(model, loss_fn, cfg_case, setup):
    """ç”¨æˆ·æ„é€ æ¥å£ï¼Œæ ¹æ® `user_type` é€‰æ‹©ä¸åŒçš„ç”¨æˆ·ç±»ã€‚"""
```
- **è¾“å…¥å‚æ•°**
  - `model`ï¼šç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆä»æœåŠ¡å™¨è·å–ï¼‰
  - `loss_fn`ï¼šæŸå¤±å‡½æ•°
  - `cfg_case`ï¼šå®éªŒé…ç½®ï¼ˆåŒ…å« `user_type`ã€æ•°æ®é›†ç­‰ä¿¡æ¯ï¼‰
  - `setup`ï¼šè®¾å¤‡ï¼ˆCPU/GPUï¼‰

- **è¿”å›å€¼**
  - `user`ï¼šä¸åŒç±»å‹çš„ç”¨æˆ·å¯¹è±¡ï¼ˆ`UserSingleStep`ã€`UserMultiStep`ã€`MultiUserAggregate`ï¼‰

---

## **ğŸ”¹ 1. å¤„ç† `local_gradient` ç”¨æˆ·**
```python
if cfg_case.user.user_type == "local_gradient":
    dataloader = construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=cfg_case.user.user_idx)
    user = UserSingleStep(model, loss_fn, dataloader, setup, idx=cfg_case.user.user_idx, cfg_user=cfg_case.user)
```

### **ğŸ“Œ è§£æ**
- **`local_gradient`ï¼ˆæœ¬åœ°æ¢¯åº¦è®¡ç®—ï¼‰**ï¼šç”¨æˆ·è®¡ç®—**ä¸€æ¬¡**æ¢¯åº¦å¹¶è¿”å›ï¼Œä¸è¿›è¡Œå¤šæ­¥ä¼˜åŒ–ã€‚
- **æ•°æ®åŠ è½½**ï¼š
  ```python
  dataloader = construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=cfg_case.user.user_idx)
  ```
  - `construct_dataloader(...)` åŠ è½½ **ç”¨æˆ· `user_idx` å¯¹åº”çš„æ•°æ®**ã€‚
  - æ¯ä¸ªç”¨æˆ·çš„æ•°æ®æ˜¯ **ç‹¬ç«‹çš„**ï¼Œä¸ä¼šå…±äº«ã€‚
  
- **åˆ›å»º `UserSingleStep` å¯¹è±¡**
  ```python
  user = UserSingleStep(model, loss_fn, dataloader, setup, idx=cfg_case.user.user_idx, cfg_user=cfg_case.user)
  ```
  - `UserSingleStep` é€‚ç”¨äº **æ¢¯åº¦æ³„éœ²æ”»å‡»ï¼ˆGradient Leakage Attackï¼‰**ï¼Œåªæ‰§è¡Œä¸€æ¬¡æ¢¯åº¦è®¡ç®—ã€‚
  - ç”¨æˆ·ä¼š **æ·±æ‹·è´æ¨¡å‹**ï¼Œé˜²æ­¢ä¿®æ”¹æœåŠ¡å™¨æä¾›çš„åŸå§‹æ¨¡å‹ã€‚

---

## **ğŸ”¹ 2. å¤„ç† `local_update` ç”¨æˆ·**
```python
elif cfg_case.user.user_type == "local_update":
    dataloader = construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=cfg_case.user.user_idx)
    user = UserMultiStep(model, loss_fn, dataloader, setup, idx=cfg_case.user.user_idx, cfg_user=cfg_case.user)
```

### **ğŸ“Œ è§£æ**
- **`local_update`ï¼ˆæœ¬åœ°å¤šæ­¥æ›´æ–°ï¼‰**ï¼šç”¨æˆ·åœ¨æœ¬åœ°æ•°æ®ä¸Šè®­ç»ƒå¤šä¸ªæ¢¯åº¦æ›´æ–°æ­¥éª¤ï¼Œè€Œä¸æ˜¯ä»…è®¡ç®—ä¸€æ¬¡æ¢¯åº¦ã€‚
- **æ•°æ®åŠ è½½**ï¼š
  ```python
  dataloader = construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=cfg_case.user.user_idx)
  ```
  - ä»ç„¶ä½¿ç”¨ `construct_dataloader(...)` **åŠ è½½ç”¨æˆ·ç‰¹å®šçš„æ•°æ®**ã€‚

- **åˆ›å»º `UserMultiStep` å¯¹è±¡**
  ```python
  user = UserMultiStep(model, loss_fn, dataloader, setup, idx=cfg_case.user.user_idx, cfg_user=cfg_case.user)
  ```
  - `UserMultiStep` é€‚ç”¨äº **æœ¬åœ°è®­ç»ƒæ›´æ–°ï¼ˆLocal Updateï¼‰**ã€‚
  - **ä¸ `UserSingleStep` ä¸åŒ**ï¼Œå®ƒä¼š **æ‰§è¡Œå¤šæ¬¡ SGD è®­ç»ƒ**ï¼ˆç±»ä¼¼äº FedAvg è®­ç»ƒæ¨¡å¼ï¼‰ã€‚
  - **å¯èƒ½æé«˜éšç§ä¿æŠ¤**ï¼Œä½†æ”»å‡»è€…ä»å¯å°è¯•æ¢å¤ç”¨æˆ·æ•°æ®ã€‚

---

## **ğŸ”¹ 3. å¤„ç† `multiuser_aggregate` ç”¨æˆ·**
```python
elif cfg_case.user.user_type == "multiuser_aggregate":
    dataloaders, indices = [], []
    for idx in range(*cfg_case.user.user_range):
        dataloaders += [construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=idx)]
        indices += [idx]
    user = MultiUserAggregate(model, loss_fn, dataloaders, setup, cfg_case.user, user_indices=indices)
```

### **ğŸ“Œ è§£æ**
- **`multiuser_aggregate`ï¼ˆå¤šç”¨æˆ·èšåˆï¼‰**ï¼šæ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·åœ¨ **ä¸åŒçš„æ•°æ®é›†ä¸Šåˆ†åˆ«è®­ç»ƒ**ï¼Œç„¶åèšåˆå®ƒä»¬çš„æ›´æ–°ã€‚
- **æ•°æ®åŠ è½½**
  ```python
  dataloaders, indices = [], []
  for idx in range(*cfg_case.user.user_range):
      dataloaders += [construct_dataloader(cfg_case.data, cfg_case.impl, user_idx=idx)]
      indices += [idx]
  ```
  - **éå† `user_range`**ï¼Œä¸º **å¤šä¸ªç”¨æˆ·** åˆ›å»º `dataloader`ã€‚
  - `indices` å­˜å‚¨æ‰€æœ‰ç”¨æˆ·çš„ç´¢å¼•ã€‚

- **åˆ›å»º `MultiUserAggregate` å¯¹è±¡**
  ```python
  user = MultiUserAggregate(model, loss_fn, dataloaders, setup, cfg_case.user, user_indices=indices)
  ```
  - `MultiUserAggregate` ä»£è¡¨ **å¤šä¸ªç”¨æˆ·çš„æ¢¯åº¦å¹³å‡**ï¼ˆæ¨¡æ‹Ÿè”é‚¦å­¦ä¹ ï¼‰ã€‚
  - é€‚ç”¨äº **èšåˆå¼è®­ç»ƒ**ï¼Œç±»ä¼¼äº **FedAvg/FedProx** è¿™æ ·çš„ FL ç®—æ³•ã€‚

---

## **ğŸ“Œ ä»£ç æ€»ç»“**
| **ç”¨æˆ·ç±»å‹ (`user_type`)** | **åˆ›å»ºçš„ç”¨æˆ·ç±»** | **è¡Œä¸º** |
|-----------------|-----------------|---------------------------|
| `local_gradient` | `UserSingleStep` | **ä»…è®¡ç®—ä¸€æ¬¡æ¢¯åº¦**ï¼Œä¸è¿›è¡Œå¤šæ­¥ä¼˜åŒ– |
| `local_update` | `UserMultiStep` | **æœ¬åœ°æ‰§è¡Œå¤šæ¬¡ SGD è®­ç»ƒ** |
| `multiuser_aggregate` | `MultiUserAggregate` | **å¤šä¸ªç”¨æˆ·åˆ†åˆ«è®­ç»ƒï¼Œæ¢¯åº¦èšåˆ** |

---

## **ğŸ“Œ ä»£ç æµç¨‹**
1. **æ£€æŸ¥ `user_type`**ï¼Œå†³å®šç”¨æˆ·è¡Œä¸ºï¼š
   - **å•æ­¥æ¢¯åº¦è®¡ç®—**ï¼ˆ`UserSingleStep`ï¼‰
   - **å¤šæ­¥æœ¬åœ°æ›´æ–°**ï¼ˆ`UserMultiStep`ï¼‰
   - **å¤šç”¨æˆ·æ¢¯åº¦èšåˆ**ï¼ˆ`MultiUserAggregate`ï¼‰

2. **åŠ è½½ç”¨æˆ·çš„æ•°æ®**
   - `construct_dataloader(...)` è¯»å–ç”¨æˆ·çš„ **æœ¬åœ°æ•°æ®**ã€‚

3. **è¿”å›ç”¨æˆ·å¯¹è±¡**
   - åˆ›å»º `user`ï¼Œå¹¶ä¼ å…¥ **æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€æ•°æ®ã€è®¾å¤‡ä¿¡æ¯**ã€‚

---

## **ğŸ“Œ ä»£ç ç¤ºä¾‹**
### **ğŸ¯ åœºæ™¯ 1ï¼šå•ç”¨æˆ·æ¢¯åº¦è®¡ç®—**
å‡è®¾ `cfg_case.user.user_type = "local_gradient"`ï¼Œ`user_idx = 1`ï¼Œåˆ™ï¼š
```python
user = construct_user(model, loss_fn, cfg_case, setup)
print(user)
```
ç­‰ä»·äºï¼š
```python
user = UserSingleStep(model, loss_fn, dataloader, setup, idx=1, cfg_user=cfg_case.user)
```
è¡¨ç¤º **ç”¨æˆ· 1** **æ‰§è¡Œå•æ­¥æ¢¯åº¦è®¡ç®—**ã€‚

---

### **ğŸ¯ åœºæ™¯ 2ï¼šæœ¬åœ°å¤šæ­¥æ›´æ–°**
å¦‚æœ `cfg_case.user.user_type = "local_update"`ï¼Œåˆ™ï¼š
```python
user = construct_user(model, loss_fn, cfg_case, setup)
```
ç­‰ä»·äºï¼š
```python
user = UserMultiStep(model, loss_fn, dataloader, setup, idx=1, cfg_user=cfg_case.user)
```
è¡¨ç¤º **ç”¨æˆ· 1 åœ¨æœ¬åœ°è¿›è¡Œå¤šè½® SGD è®­ç»ƒ**ã€‚

---

### **ğŸ¯ åœºæ™¯ 3ï¼šå¤šç”¨æˆ·æ¢¯åº¦èšåˆ**
å¦‚æœ `cfg_case.user.user_type = "multiuser_aggregate"`ï¼Œç”¨æˆ·ç´¢å¼•èŒƒå›´æ˜¯ `[0, 5]`ï¼š
```python
user = construct_user(model, loss_fn, cfg_case, setup)
```
ç­‰ä»·äºï¼š
```python
user = MultiUserAggregate(model, loss_fn, [dataloader_0, dataloader_1, ..., dataloader_4], setup, cfg_case.user, user_indices=[0, 1, 2, 3, 4])
```
è¡¨ç¤º **5 ä¸ªç”¨æˆ·åˆ†åˆ«è®¡ç®—æ¢¯åº¦ï¼Œå¹¶è¿›è¡Œæ¢¯åº¦èšåˆ**ã€‚

---

## **ğŸ“Œ ç»“è®º**
âœ… `construct_user()` æ ¹æ® `user_type` **æ„å»ºä¸åŒç±»å‹çš„ç”¨æˆ·**ã€‚  
âœ… **æœ¬åœ°å•æ­¥è®­ç»ƒ (`UserSingleStep`) é€‚ç”¨äºæ¢¯åº¦æ”»å‡»**ã€‚  
âœ… **æœ¬åœ°å¤šæ­¥è®­ç»ƒ (`UserMultiStep`) é€‚ç”¨äº FL è®­ç»ƒ**ã€‚  
âœ… **å¤šç”¨æˆ·èšåˆ (`MultiUserAggregate`) é€‚ç”¨äºæ¢¯åº¦å¹³å‡**ï¼ˆå¦‚ FedAvgï¼‰ã€‚  

ğŸ’¡ **æ€»ç»“ï¼šè¯¥å‡½æ•°æ˜¯ FL è®­ç»ƒçš„ç”¨æˆ·åˆå§‹åŒ–å…¥å£ï¼Œå†³å®šç”¨æˆ·å¦‚ä½•ä¸æœåŠ¡å™¨äº¤äº’ï¼ğŸš€**


---
---
---

# `UserMultiStep` ç±»

## **ğŸ“Œ `UserMultiStep` ç±»è¯¦ç»†è§£æ**

### **ğŸ”¹ ä½œç”¨**
- **`UserMultiStep` ç»§æ‰¿è‡ª `UserSingleStep`**ï¼Œæ‰©å±•äº† **æœ¬åœ°æ›´æ–°æ­¥éª¤çš„è®¡ç®—**ï¼ˆç”¨äº **FedAVG** åœºæ™¯ï¼‰ã€‚
- **åœ¨è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰ä¸­ï¼Œç”¨æˆ·è¿›è¡Œå¤šä¸ªæœ¬åœ°æ¢¯åº¦æ›´æ–°æ­¥éª¤**ï¼Œç„¶åå°†æ›´æ–°ä¿¡æ¯ **ï¼ˆæ¢¯åº¦ã€æ¨¡å‹å‚æ•°ç­‰ï¼‰** å‘é€ç»™æœåŠ¡å™¨ã€‚

---

## **ğŸ“Œ ä»£ç ç»“æ„**
```python
class UserMultiStep(UserSingleStep):
```
- **ç»§æ‰¿ `UserSingleStep`**ï¼š
  - `UserSingleStep` ä»…æ‰§è¡Œ **å•æ¬¡æœ¬åœ°æ¢¯åº¦æ›´æ–°**ï¼ˆFedSGDï¼‰ã€‚
  - `UserMultiStep` æ”¯æŒ **å¤šæ¬¡æœ¬åœ°æ¢¯åº¦æ›´æ–°**ï¼ˆFedAVGï¼‰ã€‚

---

## **1. `__init__()` æ„é€ å‡½æ•°**
```python
def __init__(self, model, loss, dataloader, setup, idx, cfg_user):
    """Initialize but do not propagate the cfg_case.user dict further."""
    super().__init__(model, loss, dataloader, setup, idx, cfg_user)

    self.num_local_updates = cfg_user.num_local_updates
    self.num_data_per_local_update_step = cfg_user.num_data_per_local_update_step
    self.local_learning_rate = cfg_user.local_learning_rate
    self.provide_local_hyperparams = cfg_user.provide_local_hyperparams
```
- **è°ƒç”¨ `UserSingleStep` çš„ `__init__()`** ç»§æ‰¿åŸºç¡€åŠŸèƒ½ï¼ˆå¦‚åŠ è½½æ•°æ®ã€åˆå§‹åŒ–æ¨¡å‹ç­‰ï¼‰ã€‚
- **æ–°å¢å‚æ•°**
  | å‚æ•° | ä½œç”¨ |
  |------|------|
  | `num_local_updates` | **æœ¬åœ°æ›´æ–°æ­¥æ•°**ï¼ˆæ¯è½® FL è¿­ä»£æ—¶ï¼Œæœ¬åœ°è®­ç»ƒå¤šå°‘æ¬¡ï¼‰ |
  | `num_data_per_local_update_step` | **æ¯æ¬¡æœ¬åœ°æ›´æ–°ä½¿ç”¨çš„æ•°æ®é‡** |
  | `local_learning_rate` | **æœ¬åœ°å­¦ä¹ ç‡** |
  | `provide_local_hyperparams` | **æ˜¯å¦å…±äº«æœ¬åœ°è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ›´æ–°æ­¥æ•°ï¼‰ç»™æœåŠ¡å™¨** |

---

## **2. `__repr__()`**
```python
def __repr__(self):
    n = "\n"
    return (
        super().__repr__()
        + n
        + f"""    Local FL Setup:
    Number of local update steps: {self.num_local_updates}
    Data per local update step: {self.num_data_per_local_update_step}
    Local learning rate: {self.local_learning_rate}

    Threat model:
    Share these hyperparams to server: {self.provide_local_hyperparams}
    """
    )
```
- **ä½œç”¨**
  - **ç»§æ‰¿ `UserSingleStep` çš„ `__repr__()`**
  - **æ‰“å° FL ç›¸å…³é…ç½®ä¿¡æ¯**ï¼š
    - æœ¬åœ°æ›´æ–°æ­¥æ•° (`num_local_updates`)
    - æ¯æ¬¡æ›´æ–°çš„æ•°æ®é‡ (`num_data_per_local_update_step`)
    - æœ¬åœ°å­¦ä¹ ç‡ (`local_learning_rate`)
    - æ˜¯å¦å…±äº«è¶…å‚æ•° (`provide_local_hyperparams`)

---

## **3. `compute_local_updates()`**
```python
def compute_local_updates(self, server_payload):
```
- **æ ¸å¿ƒæ–¹æ³•**ï¼Œç”¨äºï¼š
  1. **æ¥æ”¶æœåŠ¡å™¨çš„ `server_payload`ï¼ˆåŒ…å«æ¨¡å‹å‚æ•°ï¼‰**
  2. **æ‰§è¡Œå¤šä¸ªæœ¬åœ°æ›´æ–°æ­¥éª¤**
  3. **è®¡ç®—æ¢¯åº¦ï¼Œå¹¶å‘é€æ›´æ–°åçš„æ•°æ®åˆ°æœåŠ¡å™¨**

---

### **ğŸ“Œ 3.1 è®¡ç®—æœ¬åœ°æ›´æ–°**
```python
self.counted_queries += 1
user_data = self._load_data()
```
- **è®°å½•ç”¨æˆ·æŸ¥è¯¢æ¬¡æ•° `counted_queries`**
- **åŠ è½½å½“å‰ç”¨æˆ·çš„æ•°æ®**

---

### **ğŸ“Œ 3.2 è½½å…¥æœåŠ¡å™¨çš„æ¨¡å‹å‚æ•°**
```python
parameters = server_payload["parameters"]
buffers = server_payload["buffers"]

with torch.no_grad():
    for param, server_state in zip(self.model.parameters(), parameters):
        param.copy_(server_state.to(**self.setup))
    if buffers is not None:
        for buffer, server_state in zip(self.model.buffers(), buffers):
            buffer.copy_(server_state.to(**self.setup))
        self.model.eval()
    else:
        self.model.train()
```
- **ä» `server_payload` è·å–æœåŠ¡å™¨æ¨¡å‹å‚æ•° (`parameters`) å’Œ `buffers`**ã€‚
- **å¤åˆ¶ `server_state` åˆ°æœ¬åœ°æ¨¡å‹ `self.model`**ã€‚
- **å¦‚æœ `buffers` ä¸ºç©ºï¼Œåˆ™è®­ç»ƒæ¨¡å¼ (`train()`)ï¼Œå¦åˆ™æ¨ç†æ¨¡å¼ (`eval()`)**ã€‚

---

### **ğŸ“Œ 3.3 è®­ç»ƒæ¨¡å‹**
```python
optimizer = torch.optim.SGD(self.model.parameters(), lr=self.local_learning_rate)
seen_data_idx = 0
label_list = []
```
- **åˆå§‹åŒ– SGD ä¼˜åŒ–å™¨**
- **`seen_data_idx` è®°å½•å½“å‰å¤„ç†åˆ°çš„æ•°æ®ç´¢å¼•**
- **`label_list` å­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶‰åŠçš„ç±»åˆ«æ ‡ç­¾**

---

#### **ğŸ”¹ è¿­ä»£æ‰§è¡Œ `num_local_updates` æ¬¡**
```python
for step in range(self.num_local_updates):
    data = {
        k: v[seen_data_idx : seen_data_idx + self.num_data_per_local_update_step] for k, v in user_data.items()
    }
    seen_data_idx += self.num_data_per_local_update_step
    seen_data_idx = seen_data_idx % self.num_data_points
    label_list.append(data["labels"].sort()[0])

    optimizer.zero_grad()
```
- **æ¯æ¬¡è¿­ä»£**
  - **ä» `user_data` ä¸­å– `num_data_per_local_update_step` ä¸ªæ ·æœ¬**
  - **æ›´æ–° `seen_data_idx`ï¼ˆå¾ªç¯å–æ•°æ®ï¼‰**
  - **è®°å½•æ ‡ç­¾ `labels`**
  - **æ¸…ç©ºæ¢¯åº¦ (`optimizer.zero_grad()`)**

---

#### **ğŸ”¹ å‰å‘ä¼ æ’­ + è®¡ç®—æŸå¤±**
```python
data[self.data_key] = (
    data[self.data_key] + self.generator_input.sample(data[self.data_key].shape)
    if self.generator_input is not None
    else data[self.data_key]
)
outputs = self.model(**data)
loss = self.loss(outputs, data["labels"])
loss.backward()
```
- **å¦‚æœ `generator_input` å­˜åœ¨ï¼Œåˆ™å¯¹è¾“å…¥æ•°æ®æ·»åŠ å™ªå£°ï¼ˆå¦‚ DPï¼‰**
- **æ‰§è¡Œå‰å‘ä¼ æ’­ `outputs = self.model(**data)`**
- **è®¡ç®—æŸå¤± `loss = self.loss(outputs, data["labels"])`**
- **åå‘ä¼ æ’­ `loss.backward()`**

---

#### **ğŸ”¹ å¤„ç†æ¢¯åº¦ï¼ˆè£å‰ª & å·®åˆ†éšç§ï¼‰**
```python
grads_ref = [p.grad for p in self.model.parameters()]
if self.clip_value > 0:
    self._clip_list_of_grad_(grads_ref)
self._apply_differential_noise(grads_ref)
optimizer.step()
```
- **å­˜å‚¨ `grads_ref`ï¼ˆæ¢¯åº¦åˆ—è¡¨ï¼‰**
- **å¦‚æœ `clip_value > 0`ï¼Œè£å‰ªæ¢¯åº¦ï¼ˆæ¢¯åº¦è£å‰ªï¼‰**
- **åº”ç”¨å·®åˆ†éšç§å™ªå£°**
- **æ›´æ–°å‚æ•° (`optimizer.step()`)**

---

### **ğŸ“Œ 3.4 è®¡ç®—æ¢¯åº¦å¹¶è¿”å›**
```python
shared_grads = [
    (p_local - p_server.to(**self.setup)).clone().detach()
    for (p_local, p_server) in zip(self.model.parameters(), parameters)
]
shared_buffers = [b.clone().detach() for b in self.model.buffers()]
```
- **è®¡ç®— `shared_grads`ï¼ˆç”¨æˆ·æ¨¡å‹å‚æ•° - æœåŠ¡å™¨æ¨¡å‹å‚æ•°ï¼‰**
- **å°† `buffers` ä¹Ÿå…±äº«ç»™æœåŠ¡å™¨**

---

### **ğŸ“Œ 3.5 å‘é€ `metadata`ï¼ˆå…ƒæ•°æ®ï¼‰**
```python
metadata = dict(
    num_data_points=self.num_data_points if self.provide_num_data_points else None,
    labels=user_data["labels"] if self.provide_labels else None,
    local_hyperparams=dict(
        lr=self.local_learning_rate,
        steps=self.num_local_updates,
        data_per_step=self.num_data_per_local_update_step,
        labels=label_list,
    )
    if self.provide_local_hyperparams
    else None,
    data_key=self.data_key,
)
```
- **å¯é€‰åœ°å‘é€è¶…å‚æ•° (`local_hyperparams`)**
- **æä¾›æ•°æ®æ ‡ç­¾ (`labels`)ï¼ˆå¯é€‰ï¼‰**

---

### **ğŸ“Œ 3.6 è¿”å› `shared_data`**
```python
shared_data = dict(
    gradients=shared_grads, buffers=shared_buffers if self.provide_buffers else None, metadata=metadata
)
true_user_data = dict(data=user_data[self.data_key], labels=user_data["labels"], buffers=shared_buffers)

return shared_data, true_user_data
```
- **è¿”å› `shared_data`ï¼ˆæ¢¯åº¦ã€buffersã€metadataï¼‰**
- **è¿”å› `true_user_data`ï¼ˆç”¨äºåˆ†æï¼‰**

---

## **ğŸ“Œ ç»“è®º**
âœ… **æ”¯æŒæœ¬åœ°å¤šæ­¥æ›´æ–°ï¼ˆFedAVGï¼‰**  
âœ… **æœ¬åœ°è®­ç»ƒåè®¡ç®— `shared_grads`ï¼Œè¿”å›ç»™æœåŠ¡å™¨**  
âœ… **æ”¯æŒå·®åˆ†éšç§ï¼ˆæ¢¯åº¦è£å‰ª & å™ªå£°ï¼‰**  
âœ… **å¯é€‰å…±äº«è¶…å‚æ•°ã€æ ‡ç­¾ç­‰ä¿¡æ¯**  

ğŸ’¡ **æ€»ç»“ï¼š`UserMultiStep` é€‚ç”¨äºè”é‚¦å­¦ä¹ ï¼ˆFedAVGï¼‰ï¼Œæ‰§è¡Œå¤šä¸ªæœ¬åœ°è®­ç»ƒæ­¥éª¤ï¼Œæé«˜æ¨¡å‹æ”¶æ•›æ•ˆç‡ï¼ğŸš€**

# ç¬¬äºŒæ¬¡è§£è¯» `compute_local_updates`

### **å‡½æ•°è§£æï¼š`compute_local_updates`**
è¯¥å‡½æ•°çš„ä½œç”¨æ˜¯ **è®¡ç®—æœ¬åœ°æ›´æ–°ï¼ˆlocal updatesï¼‰**ï¼Œå³å®¢æˆ·ç«¯æ ¹æ®æœåŠ¡å™¨ä¸‹å‘çš„ `server_payload`ï¼ˆæœåŠ¡å™¨æ¨¡å‹å‚æ•°ï¼‰åœ¨æœ¬åœ°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿”å›è®­ç»ƒåçš„ **æ¢¯åº¦å·®åˆ†** å’Œ **å…ƒæ•°æ®** ä¾›æœåŠ¡å™¨èšåˆã€‚

---

## **1. ä»£ç ç»“æ„**
å‡½æ•°çš„æ•´ä½“é€»è¾‘å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
1. **åŠ è½½æœ¬åœ°æ•°æ®**
2. **ä»æœåŠ¡å™¨ä¸‹å‘çš„ `server_payload` æå–å‚æ•°**
3. **åŒæ­¥æ¨¡å‹å‚æ•°**
4. **æœ¬åœ°è®­ç»ƒï¼ˆåŒ…æ‹¬å‰å‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—ã€è£å‰ªã€å™ªå£°æ·»åŠ ç­‰ï¼‰**
5. **è®¡ç®—å¹¶è¿”å›ä¸æœåŠ¡å™¨ç‰ˆæœ¬çš„æ¢¯åº¦å·®åˆ†**

---

## **2. è¯¦ç»†ä»£ç è§£æ**

### **(1) è®¡æ•°å¹¶åŠ è½½æœ¬åœ°æ•°æ®**
```python
self.counted_queries += 1
user_data = self._load_data()
```
- `self.counted_queries += 1`ï¼šè®°å½•æœ¬åœ°è®¡ç®—çš„æ¬¡æ•°ï¼Œå¯èƒ½ç”¨äºç»Ÿè®¡æˆ–è€…éšç§é¢„ç®—ç®¡ç†ï¼ˆå¦‚ `DP-SGD`ï¼‰ã€‚
- `user_data = self._load_data()`ï¼šåŠ è½½ç”¨æˆ·æ•°æ®ã€‚**è¿™ä¸ªå‡½æ•° `self._load_data()` ä½ éœ€è¦æä¾›ä»£ç ï¼Œæˆ‘æ— æ³•ç¡®å®šå®ƒçš„å®ç°æ–¹å¼ã€‚**

---

### **(2) è§£ææœåŠ¡å™¨çš„ `server_payload`**
```python
parameters = server_payload["parameters"]
buffers = server_payload["buffers"]
```
- `parameters`ï¼šæœåŠ¡å™¨ä¸‹å‘çš„æ¨¡å‹å‚æ•°ï¼ˆä¸€èˆ¬æ˜¯ `state_dict` ä¸­çš„ `weights`ï¼‰ã€‚
- `buffers`ï¼šé¢å¤–çš„ `buffers` æ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œä¾‹å¦‚ `BatchNorm` çš„ `running_mean` å’Œ `running_var`ï¼‰ã€‚

---

### **(3) åŠ è½½æœåŠ¡å™¨å‚æ•°å¹¶è®¾ç½®æ¨¡å‹æ¨¡å¼**
```python
with torch.no_grad():
    for param, server_state in zip(self.model.parameters(), parameters):
        param.copy_(server_state.to(**self.setup))  # è¦†ç›–æœ¬åœ°æ¨¡å‹å‚æ•°
    if buffers is not None:
        for buffer, server_state in zip(self.model.buffers(), buffers):
            buffer.copy_(server_state.to(**self.setup))
        self.model.eval()  # å¦‚æœæœ‰ buffersï¼ˆå¦‚ BatchNormï¼‰ï¼Œè®¾ä¸º eval
    else:
        self.model.train()  # æ²¡æœ‰ buffersï¼Œä¿æŒ train æ¨¡å¼
```
- **ç›®çš„**ï¼šç¡®ä¿æœ¬åœ°æ¨¡å‹å’ŒæœåŠ¡å™¨æ¨¡å‹ **åŒæ­¥**ã€‚
- **`copy_()`**ï¼šç›´æ¥è¦†ç›– `self.model` çš„å‚æ•°ï¼Œä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚
- **æ¨¡å‹æ¨¡å¼**ï¼š
  - å¦‚æœ `buffers` **å­˜åœ¨**ï¼Œé€šå¸¸æ˜¯ `BatchNorm` ç›¸å…³å‚æ•°ï¼Œæ¨¡å‹åº”è¯¥ **ä½¿ç”¨ eval æ¨¡å¼**ï¼Œé¿å…ç»Ÿè®¡å‚æ•°æ›´æ–°ã€‚
  - å¦‚æœ `buffers` **ä¸å­˜åœ¨**ï¼Œåˆ™ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼ˆ`train()`ï¼‰ã€‚

---

### **(4) è®°å½•æ—¥å¿—**
```python
log.info(
    f"Computing user update on user {self.user_idx} in model mode: {'training' if self.model.training else 'eval'}."
)
```
è®°å½•ç”¨æˆ·ç¼–å· `self.user_idx` ä»¥åŠæ¨¡å‹å½“å‰çš„ `training` æˆ– `eval` æ¨¡å¼ï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚

---

### **(5) åˆå§‹åŒ–ä¼˜åŒ–å™¨**
```python
optimizer = torch.optim.SGD(self.model.parameters(), lr=self.local_learning_rate)
```
- ä½¿ç”¨ **éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰** ä¼˜åŒ–å™¨
- å­¦ä¹ ç‡ `lr` ç”± `self.local_learning_rate` å†³å®š

---

### **(6) æœ¬åœ°è®­ç»ƒå¾ªç¯**
```python
seen_data_idx = 0
label_list = []
for step in range(self.num_local_updates):
```
- `self.num_local_updates`ï¼šæœ¬åœ°æ›´æ–°çš„ **è¿­ä»£æ¬¡æ•°**ï¼Œå†³å®šäº†æ¯ä¸ªå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒå¤šå°‘æ­¥ã€‚
- `seen_data_idx`ï¼šç”¨äºç´¢å¼•è®­ç»ƒæ•°æ®ã€‚

#### **(6.1) è·å–å½“å‰æ‰¹æ¬¡æ•°æ®**
```python
data = {
    k: v[seen_data_idx : seen_data_idx + self.num_data_per_local_update_step] for k, v in user_data.items()
}
seen_data_idx += self.num_data_per_local_update_step
seen_data_idx = seen_data_idx % self.num_data_points
label_list.append(data["labels"].sort()[0])
```
- `self.num_data_per_local_update_step`ï¼šæœ¬åœ°æ¯ä¸€æ­¥è®­ç»ƒä½¿ç”¨çš„æ•°æ®é‡ã€‚
- **æ•°æ®å¾ªç¯**ï¼š
  - `seen_data_idx` è®°å½•æ•°æ®ç´¢å¼•ï¼Œé¿å…è¶Šç•Œï¼Œé‡‡ç”¨å–æ¨¡è¿ç®—å¾ªç¯æ•°æ®ã€‚
  - è®°å½• `label_list`ï¼Œç”¨äºåç»­ç»Ÿè®¡ï¼ˆå¦‚æœ `self.provide_labels=True`ï¼‰ã€‚

#### **(6.2) è®¡ç®—å‰å‘ä¼ æ’­**
```python
optimizer.zero_grad()
data[self.data_key] = (
    data[self.data_key] + self.generator_input.sample(data[self.data_key].shape)
    if self.generator_input is not None
    else data[self.data_key]
)
outputs = self.model(**data)
```
- **æ¸…ç©ºæ¢¯åº¦**ï¼š`optimizer.zero_grad()`
- **æ•°æ®æ‰°åŠ¨ï¼ˆå¦‚æœæœ‰ï¼‰**ï¼š
  - `self.generator_input.sample(...)` å¯èƒ½æ˜¯ **å™ªå£°ç”Ÿæˆå™¨**ï¼ˆå¦‚ DP-SGD çš„é«˜æ–¯å™ªå£°ï¼‰ã€‚
- **å‰å‘ä¼ æ’­**ï¼š
  - `self.model(**data)` è¿›è¡Œè®¡ç®—ï¼Œ`data` å¯èƒ½åŒ…å« `input_ids`, `attention_mask`, `labels` ç­‰ã€‚

#### **(6.3) è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­**
```python
loss = self.loss(outputs, data["labels"])
loss.backward()
```
- è®¡ç®—æŸå¤±ï¼š`self.loss(outputs, data["labels"])`
- åå‘ä¼ æ’­ï¼š`loss.backward()`

#### **(6.4) å¤„ç†æ¢¯åº¦**
```python
grads_ref = [p.grad for p in self.model.parameters()]
if self.clip_value > 0:
    self._clip_list_of_grad_(grads_ref)
self._apply_differential_noise(grads_ref)
optimizer.step()
```
- `grads_ref`ï¼šè·å–æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚
- **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**
  - `self._clip_list_of_grad_(grads_ref)`ï¼ˆå¦‚æœ `clip_value > 0`ï¼‰ï¼Œå¯ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
- **æ·»åŠ å·®åˆ†éšç§å™ªå£°**
  - `self._apply_differential_noise(grads_ref)`ï¼Œå¯èƒ½æ˜¯ **æ‹‰æ™®æ‹‰æ–¯å™ªå£°æˆ–é«˜æ–¯å™ªå£°**ã€‚

---

### **(7) è®¡ç®—ä¸æœåŠ¡å™¨çš„æ¢¯åº¦å·®åˆ†**
```python
shared_grads = [
    (p_local - p_server.to(**self.setup)).clone().detach()
    for (p_local, p_server) in zip(self.model.parameters(), parameters)
]
```
- **æ¢¯åº¦å·®åˆ†è®¡ç®—**ï¼š
  - è®¡ç®—æœ¬åœ°æ¨¡å‹ `p_local` ä¸æœåŠ¡å™¨æ¨¡å‹ `p_server` ä¹‹é—´çš„å‚æ•°å·®å€¼
  - `.clone().detach()` é˜²æ­¢æ¢¯åº¦ä¼ æ’­ã€‚

è¿™éƒ¨åˆ†ä»£ç è®¡ç®—çš„æ˜¯ **å‚æ•°å·®å€¼ï¼ˆparameter differenceï¼‰**ï¼Œè€Œ**ä¸æ˜¯æ¢¯åº¦ï¼ˆgradientsï¼‰**ã€‚

---

### **è§£æï¼š**
```python
shared_grads = [
    (p_local - p_server.to(**self.setup)).clone().detach()
    for (p_local, p_server) in zip(self.model.parameters(), parameters)
]
```
- `p_local` æ˜¯ **æœ¬åœ°æ¨¡å‹å‚æ•°**ï¼ˆåœ¨æœ¬åœ°æ›´æ–°åï¼‰ã€‚
- `p_server` æ˜¯ **æœåŠ¡å™¨ä¸‹å‘çš„å‚æ•°**ï¼ˆå³ `server_payload["parameters"]`ï¼‰ã€‚
- `p_server.to(**self.setup)`ï¼šå°†æœåŠ¡å™¨å‚æ•°è½¬æ¢åˆ°åˆé€‚çš„è®¾å¤‡å’Œæ•°æ®æ ¼å¼ã€‚
- `p_local - p_server`ï¼šè®¡ç®—**å‚æ•°å·®å€¼**ï¼Œè¡¨ç¤ºæœ¬åœ°å‚æ•°ç›¸å¯¹äºæœåŠ¡å™¨å‚æ•°çš„å˜åŒ–ã€‚
- `.clone().detach()`ï¼š
  - `.clone()`ï¼šåˆ›å»ºå‰¯æœ¬ï¼Œé˜²æ­¢åŸå§‹å¼ é‡è¢«ä¿®æ”¹ã€‚
  - `.detach()`ï¼šç¡®ä¿è®¡ç®—å›¾ä¸ä¼šç»§ç»­è¿½è¸ªè¿™äº›å‚æ•°ï¼Œé¿å…æ¢¯åº¦ä¼ æ’­ã€‚

---

### **æ¢¯åº¦ vs. å‚æ•°å·®å€¼**
- **æ¢¯åº¦ï¼ˆGradientï¼‰**
  - ç”± `loss.backward()` è®¡ç®—å¾—åˆ°ï¼Œå­˜å‚¨åœ¨ `p.grad` ä¸­ã€‚
  - åæ˜ äº† **æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„å˜åŒ–ç‡**ã€‚
  - è®­ç»ƒæ—¶ï¼Œæ¢¯åº¦è¢«ä¼˜åŒ–å™¨ç”¨äºæ›´æ–°å‚æ•°ï¼š
    $$ \theta = \theta - \eta \nabla L(\theta) $$

- **å‚æ•°å·®å€¼ï¼ˆParameter Differenceï¼‰**
  - è®¡ç®—æ–¹å¼æ˜¯ `p_local - p_server`ï¼Œå³**æœ¬åœ°æ›´æ–°åçš„å‚æ•°ä¸æœåŠ¡å™¨å‚æ•°çš„ç›´æ¥å·®å¼‚**ã€‚
  - åæ˜ çš„æ˜¯ **è”é‚¦å­¦ä¹ ä¸­çš„å±€éƒ¨æ¨¡å‹æ›´æ–°é‡**ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ã€‚
  - æœåŠ¡å™¨ç«¯å¯ä»¥ä½¿ç”¨è¿™äº›å·®å€¼æ¥èšåˆæœ¬åœ°æ›´æ–°ï¼Œä¾‹å¦‚ï¼š
    $$ \theta_{\text{global}} = \theta_{\text{global}} + \sum_i w_i (\theta_i - \theta_{\text{server}}) $$

---

### **æ€»ç»“**
âœ… **ç»“è®ºï¼š`shared_grads` å­˜å‚¨çš„æ˜¯å‚æ•°å·®å€¼ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ã€‚**
- å¦‚æœè¦è·å–æ¢¯åº¦ï¼Œåº”è¯¥ä½¿ç”¨ `p.grad`ã€‚
- è¯¥ä»£ç é€šå¸¸ç”¨äº**è”é‚¦å¹³å‡ï¼ˆFedAvgï¼‰**æˆ–**è”é‚¦ä¼˜åŒ–**ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼ è¾“æ¢¯åº¦ã€‚

```python
shared_buffers = [b.clone().detach() for b in self.model.buffers()]
```
- å…±äº« `buffers`ï¼Œå¦‚æœ `BatchNorm` ç›¸å…³å‚æ•°éœ€è¦å‘é€ç»™æœåŠ¡å™¨ã€‚

---

### **(8) ç»„è£…å…ƒæ•°æ®**
```python
metadata = dict(
    num_data_points=self.num_data_points if self.provide_num_data_points else None,
    labels=user_data["labels"] if self.provide_labels else None,
    local_hyperparams=dict(
        lr=self.local_learning_rate,
        steps=self.num_local_updates,
        data_per_step=self.num_data_per_local_update_step,
        labels=label_list,
    ) if self.provide_local_hyperparams else None,
    data_key=self.data_key,
)
```
- **åŒ…å«æœ¬åœ°è®­ç»ƒçš„å…ƒæ•°æ®**ï¼š
  - `num_data_points`ï¼šæ•°æ®é‡
  - `labels`ï¼šæ˜¯å¦æä¾›æ ‡ç­¾
  - `local_hyperparams`ï¼šåŒ…æ‹¬ `lr`ï¼Œè®­ç»ƒæ­¥æ•°ï¼Œæ•°æ®é‡ç­‰

---

### **(9) ç»„è£…è¿”å›å€¼**
```python
shared_data = dict(
    gradients=shared_grads,
    buffers=shared_buffers if self.provide_buffers else None,
    metadata=metadata
)
true_user_data = dict(
    data=user_data[self.data_key],
    labels=user_data["labels"],
    buffers=shared_buffers
)
return shared_data, true_user_data
```
- `shared_data`ï¼šå…±äº«ç»™æœåŠ¡å™¨çš„æ¢¯åº¦ã€buffersã€å…ƒæ•°æ®ã€‚
- `true_user_data`ï¼šåŒ…å« **ç”¨æˆ·åŸå§‹æ•°æ®**ï¼Œå¯èƒ½ç”¨äºè°ƒè¯•ã€‚

---

## **3. æ€»ç»“**
**ä¸»è¦ä½œç”¨**ï¼š
- **æœ¬åœ°è®­ç»ƒ**ï¼šåœ¨ `server_payload` æä¾›çš„æ¨¡å‹å‚æ•°åŸºç¡€ä¸Šè®­ç»ƒæœ¬åœ°æ•°æ®
- **æ¢¯åº¦æ›´æ–°**ï¼šè®¡ç®—æœ¬åœ° **æ¢¯åº¦å·®åˆ†**
- **éšç§ä¿æŠ¤**ï¼š
  - **æ¢¯åº¦è£å‰ª**
  - **å·®åˆ†éšç§å™ªå£°**
- **è¿”å›æœ¬åœ°è®­ç»ƒç»“æœ** ä¾›æœåŠ¡å™¨ç«¯èšåˆã€‚

# `buffer`å’Œ`parameter`çš„åŒºåˆ«

## **1. `model.parameters()` å’Œ `model.buffers()` çš„å…³ç³»ä¸ç”¨é€”**

åœ¨ PyTorch ä¸­ï¼Œ`model.parameters()` å’Œ `model.buffers()` ä¸»è¦ç”¨äºç®¡ç†**ç¥ç»ç½‘ç»œçš„æƒé‡ã€æ¢¯åº¦ã€ä»¥åŠæ— æ¢¯åº¦çš„çŠ¶æ€ä¿¡æ¯**ã€‚å®ƒä»¬æœ‰ä¸åŒçš„ä½œç”¨å’Œç‰¹æ€§ï¼š

| **ç±»åˆ«**              | **æ¥æº**                   | **æ¢¯åº¦** | **ä¼˜åŒ–å™¨æ›´æ–°** | **å­˜å‚¨æ–¹å¼** | **ç”¨é€”** |
|----------------------|--------------------------|---------|--------------|-------------|---------|
| `model.parameters()` | `torch.nn.Module` ä¸­çš„ `nn.Parameter`  | æœ‰æ¢¯åº¦  | **æ˜¯**  | `state_dict` ä¸­çš„ `parameters` | **è®­ç»ƒæƒé‡ï¼Œå¦‚å·ç§¯æ ¸ã€å…¨è¿æ¥å±‚çš„æƒé‡ç­‰** |
| `model.buffers()`    | `torch.nn.Module` ä¸­çš„ `self.register_buffer()`  | æ— æ¢¯åº¦  | **å¦**  | `state_dict` ä¸­çš„ `buffers` | **ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚ `BatchNorm` çš„ `running_mean` å’Œ `running_var`** |

---

## **2. `model.parameters()`**
### **ï¼ˆ1ï¼‰å®šä¹‰**
- `model.parameters()` è¿”å› **æ‰€æœ‰å‚ä¸æ¢¯åº¦è®¡ç®—çš„å‚æ•°**ï¼ˆå³ `requires_grad=True`ï¼‰ã€‚
- è¿™äº›å‚æ•°é€šå¸¸æ˜¯ `nn.Module` é‡Œçš„ `nn.Parameter`ï¼Œå¯ä»¥è¢«ä¼˜åŒ–å™¨ï¼ˆå¦‚ `SGD`ã€`Adam`ï¼‰æ›´æ–°ã€‚

### **ï¼ˆ2ï¼‰ä½œç”¨**
- **ç”¨äºæ¨¡å‹è®­ç»ƒ**ï¼šåå‘ä¼ æ’­æ—¶æ›´æ–°è¿™äº›å‚æ•°ã€‚
- **è¢« `optimizer` è®¿é—®å’Œä¼˜åŒ–**ã€‚

### **ï¼ˆ3ï¼‰ç¤ºä¾‹**
```python
import torch
import torch.nn as nn

# å®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 5)  # 10ä¸ªè¾“å…¥ï¼Œ5ä¸ªè¾“å‡º

model = MyModel()

# æŸ¥çœ‹å‚æ•°
for param in model.parameters():
    print(param.shape, param.requires_grad)
```
**è¾“å‡º**
```
torch.Size([5, 10]) True  # fc.weight
torch.Size([5]) True  # fc.bias
```
- `fc.weight` å’Œ `fc.bias` æ˜¯ `model.parameters()` çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸” `requires_grad=True`ã€‚

### **ï¼ˆ4ï¼‰ä¼˜åŒ–å™¨å¦‚ä½•ä½¿ç”¨**
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

# è®­ç»ƒæ­¥éª¤
input_data = torch.randn(3, 10)  # batch_size=3, features=10
target = torch.randn(3, 5)  # ç›®æ ‡å€¼

optimizer.zero_grad()  # æ¸…é™¤æ—§çš„æ¢¯åº¦
output = model(input_data)  # å‰å‘ä¼ æ’­
loss = loss_fn(output, target)  # è®¡ç®—æŸå¤±
loss.backward()  # åå‘ä¼ æ’­
optimizer.step()  # æ›´æ–°æƒé‡
```
- `optimizer.step()` åªä¼š **æ›´æ–° `model.parameters()`**ï¼Œä¸ä¼šå½±å“ `model.buffers()`ã€‚

---

## **3. `model.buffers()`**
### **ï¼ˆ1ï¼‰å®šä¹‰**
- `model.buffers()` è¿”å› **ä¸ä¼šè¢«ä¼˜åŒ–çš„å¼ é‡**ï¼ˆå³ `requires_grad=False`ï¼‰ã€‚
- è¿™äº›å¼ é‡é€šå¸¸ç”± `self.register_buffer(name, tensor)` åˆ›å»ºï¼Œå­˜å‚¨**æ¨¡å‹çš„é¢å¤–çŠ¶æ€ä¿¡æ¯**ï¼Œè€Œä¸æ˜¯å­¦ä¹ å‚æ•°ã€‚

### **ï¼ˆ2ï¼‰ä½œç”¨**
- **ç”¨äºæ¨ç†ä½†ä¸è®­ç»ƒ**ï¼ˆå¦‚ `BatchNorm` çš„ `running_mean`ï¼‰ã€‚
- **ä¸ä¼šè¢« `optimizer` æ›´æ–°**ï¼Œä½†å¯ä»¥åœ¨ `model.eval()` å’Œ `model.train()` ä¹‹é—´åˆ‡æ¢çŠ¶æ€ã€‚

### **ï¼ˆ3ï¼‰ç¤ºä¾‹**
```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 5)
        self.register_buffer("running_var", torch.ones(5))  # æ³¨å†Œ buffer

model = MyModel()

# æŸ¥çœ‹ buffers
for buf in model.buffers():
    print(buf.shape, buf.requires_grad)
```
**è¾“å‡º**
```
torch.Size([5]) False  # running_var
```
- `running_var` è¢«æ³¨å†Œä¸º `buffer`ï¼Œä¸ä¼šè®¡ç®—æ¢¯åº¦ (`requires_grad=False`)ã€‚

### **ï¼ˆ4ï¼‰ä¸ `BatchNorm` å…³ç³»**
åœ¨ `BatchNorm` å±‚ä¸­ï¼Œ`running_mean` å’Œ `running_var` ä¸æ˜¯ `parameters()`ï¼Œè€Œæ˜¯ `buffers()`ï¼š
```python
bn = nn.BatchNorm2d(3)  # 3é€šé“
for buf in bn.buffers():
    print(buf.shape, buf.requires_grad)
```
**è¾“å‡º**
```
torch.Size([3]) False  # running_mean
torch.Size([3]) False  # running_var
```
- è¿™äº› `buffers` **ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°**ï¼Œä½† **ä¼šåœ¨ `model.train()` æœŸé—´æ›´æ–°**ï¼Œè€Œ `model.eval()` æ—¶ä¿æŒå›ºå®šã€‚

### **ï¼ˆ5ï¼‰å¦‚ä½•æ‰‹åŠ¨æ›´æ–° buffer**
```python
model.running_var += 0.1  # æ‰‹åŠ¨ä¿®æ”¹ buffer
```
- ç”±äº `buffer` ä¸æ˜¯ `parameter`ï¼Œå®ƒéœ€è¦ **æ‰‹åŠ¨æ›´æ–°**ï¼Œæˆ–è€…åœ¨ `forward()` é‡Œæ›´æ–°ã€‚

---

## **4. `model.parameters()` vs `model.buffers()` çš„åŒºåˆ«**
### **(1) æ˜¯å¦å‚ä¸è®­ç»ƒ**
| **ç±»å‹**            | **æ˜¯å¦è®¡ç®—æ¢¯åº¦** | **æ˜¯å¦æ›´æ–°** | **ä½¿ç”¨æ–¹å¼** |
|--------------------|--------------|------------|------------|
| `model.parameters()` | æ˜¯            | **æ˜¯**      | è®­ç»ƒæ¨¡å‹ |
| `model.buffers()`    | å¦            | **å¦**      | å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯ |

### **(2) åœ¨ `state_dict` ä¸­çš„åŒºåˆ«**
```python
print(model.state_dict().keys())
```
- `parameters()` å­˜åœ¨äº `state_dict` çš„ `parameters` éƒ¨åˆ†ã€‚
- `buffers()` å­˜åœ¨äº `state_dict` çš„ `buffers` éƒ¨åˆ†ã€‚

---

## **5. `compute_local_updates` é‡Œ `parameters` å’Œ `buffers` çš„ç”¨é€”**
```python
for param, server_state in zip(self.model.parameters(), parameters):
    param.copy_(server_state.to(**self.setup))
```
- **ä½œç”¨**ï¼šå°†æœåŠ¡å™¨ç«¯çš„ `parameters` **åŒæ­¥åˆ°æœ¬åœ°æ¨¡å‹**ã€‚

```python
if buffers is not None:
    for buffer, server_state in zip(self.model.buffers(), buffers):
        buffer.copy_(server_state.to(**self.setup))
    self.model.eval()  # å¦‚æœæœ‰ buffersï¼Œåˆ™è¿›å…¥æ¨ç†æ¨¡å¼
```
- **ä½œç”¨**ï¼šå¦‚æœæœåŠ¡å™¨ç«¯æä¾› `buffers`ï¼Œåˆ™åŒæ­¥åˆ°æœ¬åœ° `buffers`ï¼Œå¹¶ **åˆ‡æ¢åˆ° `eval` æ¨¡å¼**ã€‚

```python
shared_grads = [
    (p_local - p_server.to(**self.setup)).clone().detach()
    for (p_local, p_server) in zip(self.model.parameters(), parameters)
]
```
- **ä½œç”¨**ï¼šè®¡ç®— `parameters` çš„æ¢¯åº¦å·®åˆ†ï¼Œä½œä¸º **è”é‚¦å­¦ä¹ ä¸­çš„æœ¬åœ°æ›´æ–°**ã€‚

```python
shared_buffers = [b.clone().detach() for b in self.model.buffers()]
```
- **ä½œç”¨**ï¼šæ”¶é›† `buffers`ï¼Œå¦‚æœ `provide_buffers=True`ï¼Œåˆ™è¿”å›ç»™æœåŠ¡å™¨ã€‚

---

## **6. ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ `parameters()` vs `buffers()`**
| **ä½¿ç”¨åœºæ™¯**            | **ç”¨ `parameters()` è¿˜æ˜¯ `buffers()`** | **ç¤ºä¾‹** |
|-----------------------|---------------------------------|-------|
| **è®­ç»ƒæƒé‡ã€åç½®** | `parameters()` | `self.fc.weight` |
| **å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯** | `buffers()` | `BatchNorm.running_mean` |
| **éœ€è¦ä¼˜åŒ–å™¨æ›´æ–°çš„å‚æ•°** | `parameters()` | `self.conv.weight` |
| **å­˜å‚¨ä¸ä¼šä¼˜åŒ–çš„æ•°æ®** | `buffers()` | `self.register_buffer("variance", torch.ones(3))` |

---

## **7. æ€»ç»“**
- **`parameters()`**ï¼š
  - è´Ÿè´£å­˜å‚¨ **å¯è®­ç»ƒå‚æ•°**ï¼ˆå¦‚ `weights` å’Œ `bias`ï¼‰ã€‚
  - è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨è¿›è¡Œæ›´æ–°ã€‚
  - **ç”¨äº `forward`ï¼Œå½±å“æ¨¡å‹è¾“å‡º**ã€‚

- **`buffers()`**ï¼š
  - å­˜å‚¨ **ä¸è®¡ç®—æ¢¯åº¦çš„çŠ¶æ€ä¿¡æ¯**ï¼ˆå¦‚ `BatchNorm` çš„ `running_mean`ï¼‰ã€‚
  - **ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°**ï¼Œä½†å¯èƒ½åœ¨ `train()` æ¨¡å¼ä¸‹åŠ¨æ€å˜åŒ–ã€‚
  - **ç”¨äº `forward`ï¼Œä½†ä¸å½±å“æ¢¯åº¦è®¡ç®—**ã€‚

åœ¨ `compute_local_updates` é‡Œï¼š
- `parameters` **ç”¨äºæœ¬åœ°è®­ç»ƒå’Œæ¢¯åº¦æ›´æ–°**ã€‚
- `buffers` **ç”¨äºåŒæ­¥æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯**ï¼Œå½±å“ `BatchNorm` å±‚çš„è¡Œä¸ºã€‚