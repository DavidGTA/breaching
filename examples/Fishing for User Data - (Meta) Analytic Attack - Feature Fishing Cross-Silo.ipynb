{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebef44a",
   "metadata": {},
   "source": [
    "# Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756fc5f",
   "metadata": {},
   "source": [
    "This notebook shows an example for a **arbitrary batch image gradient inversion** as described in \"Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification\". The setting is a ViT variant (as in the April paper) and the federated learning algorithm is **fedSGD** in a **cross-silo** setting.\n",
    "\n",
    "Paper URL: https://arxiv.org/abs/2202.00580"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a6eea",
   "metadata": {},
   "source": [
    "This variant fishes for user data of a single target user by refining an estimate of the user's data distribution over a series of queries. The attacker constructs a binary tree over successive queries until a single data point from the user side is successfully isolated. On average, this should take $\\log(n)$ queries for users with $n$ data points from the class that is currently attacked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107d723",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "Federated learning (FL) has rapidly risen in popularity due to its promise of privacy and efficiency. Previous works have exposed privacy vulnerabilities in the FL pipeline by recovering user data from gradient updates. However, existing attacks fail to address realistic settings because they either 1) require a `toy' settings with very small batch sizes, or 2) require unrealistic and conspicuous architecture modifications. We introduce a new strategy that dramatically elevates existing attacks to operate on batches of arbitrarily large size, and without architectural modifications. Our model-agnostic strategy only requires modifications to the model parameters sent to the user, which is a realistic threat model in many scenarios. We demonstrate the strategy in challenging large-scale settings, obtaining high-fidelity data extraction in both cross-device and cross-silo federated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcd6cb",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import breaching\n",
    "except ModuleNotFoundError:\n",
    "    # You only really need this safety net if you want to run these notebooks directly in the examples directory\n",
    "    # Don't worry about this if you installed the package or moved the notebook to the main directory.\n",
    "    import os; os.chdir(\"..\")\n",
    "    import breaching\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Redirects logs directly into the jupyter notebook\n",
    "import logging, sys\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)], format='%(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5e214",
   "metadata": {},
   "source": [
    "### Initialize cfg object and system setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd663b",
   "metadata": {},
   "source": [
    "This will load the full configuration object. This includes the configuration for the use case and threat model as `cfg.case` and the hyperparameters and implementation of the attack as `cfg.attack`. All parameters can be modified below, or overriden with `overrides=` as if they were cmd-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = breaching.get_config(overrides=[\"case/server=malicious-fishing\", \"attack=april_analytic\"])\n",
    "          \n",
    "device = torch.device(f'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.backends.cudnn.benchmark = cfg.case.impl.benchmark\n",
    "setup = dict(device=device, dtype=getattr(torch, cfg.case.impl.dtype))\n",
    "setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c5fb1",
   "metadata": {},
   "source": [
    "### Modify config options here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0764ef",
   "metadata": {},
   "source": [
    "You can use `.attribute` access to modify any of these configurations for the attack, or the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac118ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.case.model = \"vit_small_april\"\n",
    "\n",
    "cfg.case.data.partition = \"unique-class\" # This is the worst-case for the attack, as each user owns a unique class\n",
    "cfg.case.user.num_data_points = 50 # maximum in the validation set. Feel free to load the training set and increase\n",
    "cfg.case.user.user_idx = 1 # goldfish\n",
    "\n",
    "cfg.case.user.provide_labels = True # Mostly out of convenience\n",
    "cfg.case.server.target_cls_idx = 0 # Which class to attack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f64389",
   "metadata": {},
   "source": [
    "### Instantiate all parties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2272f",
   "metadata": {},
   "source": [
    "The following lines generate \"server, \"user\" and \"attacker\" objects and print an overview of their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "user, server, model, loss_fn = breaching.cases.construct_case(cfg.case, setup)\n",
    "attacker = breaching.attacks.prepare_attack(server.model, server.loss, cfg.attack, setup)\n",
    "breaching.utils.overview(server, user, attacker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c0ad6",
   "metadata": {},
   "source": [
    "### Simulate an attacked FL protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e43d4",
   "metadata": {},
   "source": [
    "This attack runs a modified server protocol, which is able to query the user multiple times. We record the number of queries and later return it from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[shared_data], [server_payload], true_user_data = server.run_protocol(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c68628",
   "metadata": {},
   "outputs": [],
   "source": [
    "user.plot(true_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17255c5a",
   "metadata": {},
   "source": [
    "### Now reconstruct  a single \"fished\" user data point:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82360c14",
   "metadata": {},
   "source": [
    "Now we launch the attack, reconstructing user data based on only the `server_payload` and the `shared_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a32fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_user_data, stats = attacker.reconstruct([server_payload], [shared_data], \n",
    "                                                      server.secrets, dryrun=cfg.dryrun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4943f",
   "metadata": {},
   "source": [
    "Next we'll evaluate metrics, comparing the `reconstructed_user_data` to the `true_user_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = breaching.analysis.report(reconstructed_user_data, true_user_data, [server_payload], \n",
    "                                    server.model, order_batch=True, compute_full_iip=False, \n",
    "                                    cfg_case=cfg.case, setup=setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200797e",
   "metadata": {},
   "source": [
    "And finally, we also plot the reconstructed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "user.plot(reconstructed_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from breaching.cases.malicious_modifications.classattack_utils import print_gradients_norm, cal_single_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_gradients, single_losses = cal_single_gradients(user.model, loss_fn, true_user_data)\n",
    "print_gradients_norm(single_gradients, single_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb085f",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* You can use `cal_single_gradients` and `print_gradients_norm` from `malicious_modifications.classattack_utils` to verify that only one of the user data points has a non-neglible gradient norm\n",
    "* We spend an additional first query here to verify the labels of this user before launching the actual attack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
