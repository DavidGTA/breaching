{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebef44a",
   "metadata": {},
   "source": [
    "# Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756fc5f",
   "metadata": {},
   "source": [
    "This notebook shows an example for the threat model and attack described in \"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models\n",
    "\". This example deviates from the other \"honest-but-curious\" server models and investigates a malicious server that may send malicious server updates. The attack succeeds for a range of common transformer architectures and works merely by sending a single malicious query to the user model.\n",
    "\n",
    "In this notebook, we attack the commonly used BERT model (`bert-base-uncased` from the huggingface implementation).\n",
    "\n",
    "\n",
    "\n",
    "Paper URL: https://arxiv.org/abs/2201.12675"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd1107",
   "metadata": {},
   "source": [
    "### Abstract:\n",
    "A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7676c5",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import breaching\n",
    "except ModuleNotFoundError:\n",
    "    # You only really need this safety net if you want to run these notebooks directly in the examples directory\n",
    "    # Don't worry about this if you installed the package or moved the notebook to the main directory.\n",
    "    import os; os.chdir(\"..\")\n",
    "    import breaching\n",
    "    \n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Redirects logs directly into the jupyter notebook\n",
    "import logging, sys\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)], format='%(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5e214",
   "metadata": {},
   "source": [
    "### Initialize cfg object and system setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd663b",
   "metadata": {},
   "source": [
    "This will load the full configuration object. This includes the configuration for the use case and threat model as `cfg.case` and the hyperparameters and implementation of the attack as `cfg.attack`. All parameters can be modified below, or overriden with `overrides=` as if they were cmd-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = breaching.get_config(overrides=[\"attack=decepticon\", \"case=9_bert_training\", \n",
    "                                     \"case/server=malicious-transformer\"])\n",
    "          \n",
    "device = torch.device('cpu')\n",
    "torch.backends.cudnn.benchmark = cfg.case.impl.benchmark\n",
    "setup = dict(device=device, dtype=getattr(torch, cfg.case.impl.dtype))\n",
    "setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c5fb1",
   "metadata": {},
   "source": [
    "### Modify config options here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0764ef",
   "metadata": {},
   "source": [
    "You can use `.attribute` access to modify any of these configurations for the attack, or the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac118ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.case.user.num_data_points = 1 # How many sentences?\n",
    "cfg.case.user.user_idx = 1 # From which user?\n",
    "cfg.case.data.shape = [512] # This is the sequence length\n",
    "\n",
    "cfg.case.server.provide_public_buffers = True # Send server signal to disable dropout\n",
    "cfg.case.server.has_external_data = True  # Not strictly necessary, but could also use random text (see Appendix)\n",
    "cfg.case.data.tokenizer = \"bert-base-uncased\"\n",
    "cfg.case.model = \"bert-base-uncased\" # Could also choose \"bert-sanity-check\" which contains ReLU activations\n",
    "cfg.case.server.pretrained = False\n",
    "\n",
    "\n",
    "## Attack hyperparameters:\n",
    "\n",
    "# Server side:\n",
    "cfg.case.server.param_modification.reset_embedding=True\n",
    "cfg.case.server.param_modification.v_length = 32 # Length of the sentence component\n",
    "cfg.case.server.param_modification.eps = 1e-8\n",
    "cfg.case.server.param_modification.measurement_scale=1e8 # Circumvent GELU\n",
    "cfg.case.server.param_modification.imprint_sentence_position = 0\n",
    "cfg.case.server.param_modification.softmax_skew = 1e8\n",
    "cfg.case.server.param_modification.sequence_token_weight = 1\n",
    "\n",
    "\n",
    "# Attacker side:\n",
    "\n",
    "# this option requires installation of `k-means-constrained` which can be tricky:\n",
    "# If this doesn't work for you, falling back to \"dynamic-threshold\" is still a decent option.\n",
    "cfg.attack.sentence_algorithm = \"k-means\" \n",
    "cfg.attack.token_strategy=\"embedding-norm\" # can also do \"mixed\" for BERT\n",
    "cfg.attack.embedding_token_weight=0.25 # This can improve performance slightly for long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f64389",
   "metadata": {},
   "source": [
    "### Instantiate all parties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71983edf",
   "metadata": {},
   "source": [
    "The following lines generate \"server, \"user\" and \"attacker\" objects and print an overview of their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "user, server, model, loss_fn = breaching.cases.construct_case(cfg.case, setup)\n",
    "attacker = breaching.attacks.prepare_attack(server.model, server.loss, cfg.attack, setup)\n",
    "breaching.utils.overview(server, user, attacker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c0ad6",
   "metadata": {},
   "source": [
    "### Simulate an attacked FL protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058bcc2",
   "metadata": {},
   "source": [
    "This exchange is a simulation of a single query in a federated learning protocol. The server sends out a `server_payload` and the user computes an update based on their private local data. This user update is `shared_data` and contains, for example, the parameter gradient of the model in the simplest case. `true_user_data` is also returned by `.compute_local_updates`, but of course not forwarded to the server or attacker and only used for (our) analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_payload = server.distribute_payload()\n",
    "shared_data, true_user_data = user.compute_local_updates(server_payload)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c68628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user.print(true_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17255c5a",
   "metadata": {},
   "source": [
    "### Reconstruct user data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e8cd6",
   "metadata": {},
   "source": [
    "Now we launch the attack, reconstructing user data based on only the `server_payload` and the `shared_data`. \n",
    "\n",
    "For this attack, we also share secret information from the malicious server with the attack (`server.secrets`), which here is the location and structure of the imprint block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a32fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_user_data, stats = attacker.reconstruct([server_payload], [shared_data], server.secrets, \n",
    "                                                      dryrun=cfg.dryrun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c35e12",
   "metadata": {},
   "source": [
    "Next we'll evaluate metrics, comparing the `reconstructed_user_data` to the `true_user_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = breaching.analysis.report(reconstructed_user_data, true_user_data, [server_payload], \n",
    "                                    server.model, order_batch=True, compute_full_iip=False, \n",
    "                                    cfg_case=cfg.case, setup=setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920aca0",
   "metadata": {},
   "source": [
    "And finally, we also plot the reconstructed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f4a84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user.print(reconstructed_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_user_data[\"confidence\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed41be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user.print_with_confidence(reconstructed_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062c422",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user.print_and_mark_correct(reconstructed_user_data, true_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adeafc",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* There are a variety of hyperparameters to the attack which are set to reasonable defaults. Performance of the attack could be improved in some unusual use cases (datasets or models) by tuning these parameters further.\n",
    "* In this example, dropout is disabled under the assumption that this is a parameter that can be controlled in the server update. The optimal attack simply disables dropout. However, the attack can still succeed when dropout is enforced by the user, albeit with a minor loss in reconstruction quality.\n",
    "* This example also assumes complete freedom to choose the parameter vector, for this reason we circumvent the smooth part of the GELU activation with a \"very\" large measurement vector magnitude. This is arguably excessive for only a small gain in accuracy. A similar argument can be made for the default softmax skew value.\n",
    "* We also want to re-emphasize that the design space of these parameter modification attacks is large. A defense against the specific parameter modification described here is unlikely to be safe in general!\n",
    "* Token recovery is much easier when the embedding is randomly initialized. Here we explicitely re-initialize the BERT embedding to improve label accuracy.\n",
    "* The embedding gradient is not strictly necessary for the attack. The attack can also be run with `token_strategy=None`, in which case the embedding gradient is entirely disregarded (and does not have to be learnable).\n",
    "* In our setup it made the most sense to consider the `[MASK]` tokens as part of the input (and this is also how the metrics are scored). However, the contents of the masked values are actually also leaked and are retrievable from the gradient of the decoder bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
