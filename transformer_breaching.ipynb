{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b546f8c",
   "metadata": {},
   "source": [
    "# Parameter attacks on Transformers? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99327874",
   "metadata": {},
   "source": [
    "Following the tutorial at https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083de34",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4fef",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af97df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a979e4",
   "metadata": {},
   "source": [
    "### Data stuff (boring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 1\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07e93f",
   "metadata": {},
   "source": [
    "### Here define the sequence length we want to consider (don't make too long ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(bptt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c70d0b",
   "metadata": {},
   "source": [
    "### Instantiate transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33996ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0 # Kinda need this to be zero :/\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ce27b",
   "metadata": {},
   "source": [
    "### Get some sample sentence just to see what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = get_batch(train_data, 100)\n",
    "data.shape\n",
    "print('\\n'.join([' '.join(vocab.lookup_tokens(data[:, i].tolist())) for i in range(data.shape[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114df50",
   "metadata": {},
   "source": [
    "## Attack setup starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2736043",
   "metadata": {},
   "source": [
    "### First, let's define our linear measurement - some Gaussian vector here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = torch.randn(200)\n",
    "std, mu = torch.std_mean(weights)\n",
    "measurement = (weights - mu) / std / math.sqrt(200) # Here's our linear measurement\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data)\n",
    "# Above we set the attention head to be zero. Probably should use these somehow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71e41a",
   "metadata": {},
   "source": [
    "### Setting the second linear layer to just all zeros except for first row helps? Need to figure out why ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data)\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0] = torch.ones_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0])\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.bias.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bf4b3",
   "metadata": {},
   "source": [
    "### Now let's get the feature statistics for our random measurement vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_distribution(model):\n",
    "        \"\"\"Compute the mean and std of the feature layer of the given network.\"\"\"\n",
    "        features = dict()\n",
    "        dtype = torch.float\n",
    "        device = torch.device('cuda')\n",
    "        setup = dict(device=device)\n",
    "        def named_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                features[name] = input[0]\n",
    "            return hook_fn\n",
    "        \n",
    "        for name, module in list(model.named_modules()):\n",
    "            if name == 'transformer_encoder.layers.0.linear1':\n",
    "                print(f'In Linear: {name}')\n",
    "                hook = module.register_forward_hook(named_hook(name))\n",
    "                feature_layer_name = name\n",
    "                break\n",
    "        feats = []\n",
    "        feats_before = []\n",
    "        model.train()\n",
    "        model.to(**setup)\n",
    "        print(f'Computing feature distribution before the {feature_layer_name} layer from external data.')\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "        model.eval()\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "            feats.append(features[feature_layer_name].detach().view(inputs.shape[0]*inputs.shape[1], -1).clone().cpu())\n",
    "        std, mu = torch.std_mean(torch.mm(torch.cat(feats), measurement.unsqueeze(1)).squeeze())\n",
    "        print(f'Feature mean is {mu.item()}, feature std is {std.item()}.')\n",
    "        model.eval()\n",
    "        model.cpu()\n",
    "        hook.remove()\n",
    "\n",
    "        return std, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a275",
   "metadata": {},
   "source": [
    "### Now we'll construct the imprint weights. Only for the first linear layer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def _get_bins(mean, std, num_bins):\n",
    "    bins = []\n",
    "    mass_per_bin = 1 / (num_bins)\n",
    "    bins.append(-10)  # -Inf is not great here, but NormalDist(mu=0, sigma=1).cdf(10) approx 1\n",
    "    for i in range(1, num_bins):\n",
    "        bins.append(NormalDist().inv_cdf(i * mass_per_bin)*std + mean)\n",
    "    return bins\n",
    "\n",
    "def _make_biases(bias_layer, bins):\n",
    "    new_biases = torch.zeros_like(bias_layer.data)\n",
    "    for i in range(new_biases.shape[0]):\n",
    "        new_biases[i] = -bins[i]\n",
    "    return new_biases\n",
    "\n",
    "feature_std, feature_mean = feature_distribution(model)\n",
    "bins = _get_bins(feature_mean, feature_std, model.d_model)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.weight.data = measurement.repeat(model.d_model, 1)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.bias.data = _make_biases(getattr(model.transformer_encoder.layers, '0').linear1.bias, bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e91db",
   "metadata": {},
   "source": [
    "### Get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "inputs, targets = get_batch(train_data, 100)\n",
    "#inputs, targets = get_batch(train_data, 0)\n",
    "inputs.cuda()\n",
    "model.cuda()\n",
    "model.zero_grad()\n",
    "output = model(inputs, src_mask)\n",
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08bc4e",
   "metadata": {},
   "source": [
    "## Actual attack starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868ee82",
   "metadata": {},
   "source": [
    "### Now get the bag of words, as well as the reconstructed positionally encoded stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_tokens = ((model.encoder.weight.grad != 0).sum(dim=1) > 0).nonzero().squeeze() # Bag of words tokens\n",
    "weight_grad = getattr(model.transformer_encoder.layers, '0').linear1.weight.grad.detach().clone().cpu()\n",
    "bias_grad = getattr(model.transformer_encoder.layers, '0').linear1.bias.grad.detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed positionally encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4097d2",
   "metadata": {},
   "source": [
    "### Now the messy stuff ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the features of our bag of words sans positional encoding\n",
    "no_pos = getattr(model.transformer_encoder.layers, '0').norm1((model.encoder(leaked_tokens) * math.sqrt(model.d_model))).cpu()\n",
    "with_pos = recs # Here are those same features, but with positional encodings (stuff we reconstructed)\n",
    "\n",
    "import numpy as np\n",
    "indcs = []\n",
    "corrs = torch.zeros((len(no_pos), len(with_pos))) \n",
    "\n",
    "# We need to find out what word led to what positionally encoded representation. \n",
    "# Let's try the naive greedy search for correlations between no_pos and with_pos as defined above\n",
    "for i, no_p in enumerate(no_pos):\n",
    "    max_corr = 0\n",
    "    for j, with_p in enumerate(with_pos):\n",
    "        val = np.corrcoef(np.array([no_p.detach().numpy(), with_p.detach().numpy()]))[0,1]\n",
    "        corrs[i,j] = val\n",
    "\n",
    "# Greedy search to find which positionally-encoded vector associates with un-positionally-encoded vector\n",
    "order = []\n",
    "for i in range(len(no_pos)):\n",
    "    row, col = torch.argmax(corrs) // corrs.shape[1], torch.argmax(corrs) % corrs.shape[1]\n",
    "    order.append((row.item(), col.item()))\n",
    "    corrs[row,:] = -1\n",
    "    corrs[:, col] = -1\n",
    "\n",
    "order = sorted(order, key=lambda x: x[1])\n",
    "\n",
    "# Now let's re-sort the tokens by this order\n",
    "sorted_tokens1 = [leaked_tokens[order[i][0]] for i in range(len(order))]\n",
    "\n",
    "# Now that we've 'lined-up' the pos-encoded features with non-pos-encoded features, let's subtract the two\n",
    "# to get some 'faux' positions (layer norm means they aren't exact).\n",
    "estimated_pos = torch.stack([with_pos[order[i][1]] - no_pos[order[i][0]] for i in range(len(order))])\n",
    "\n",
    "# Now let's get just the additive part of the positional encoding\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# We'll do another greedy search, but now it's on the positions of the tokens\n",
    "# First calculate the correlations between (faux) estimated_pos and the (real) just_pos terms\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "\n",
    "# Greedily gobble up the most correlated positions :) \n",
    "pos_order = []\n",
    "for i in range(len(estimated_pos)):\n",
    "    row, col = torch.argmax(order_coeffs) // order_coeffs.shape[1], torch.argmax(order_coeffs) % order_coeffs.shape[1]\n",
    "    pos_order.append((row.item(), col.item()))\n",
    "    order_coeffs[row,:] = -1\n",
    "    order_coeffs[:, col] = -1\n",
    "\n",
    "# Finally, reorder one more time ...\n",
    "pos_order = sorted(pos_order, key=lambda x: x[1])\n",
    "final_sorted_tokens = [sorted_tokens1[pos_order[i][0]] for i in range(len(pos_order))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8f558",
   "metadata": {},
   "source": [
    "### Let's see how we did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.lookup_tokens([x[0] for x in inputs])) # The true sentence\n",
    "print(vocab.lookup_tokens(final_sorted_tokens)) # What we reconstruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
