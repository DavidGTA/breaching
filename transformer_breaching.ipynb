{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b546f8c",
   "metadata": {},
   "source": [
    "# Parameter attacks on Transformers? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99327874",
   "metadata": {},
   "source": [
    "Following the tutorial at https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a7090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083de34",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4fef",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af97df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a979e4",
   "metadata": {},
   "source": [
    "### Data stuff (boring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ba4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 2\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07e93f",
   "metadata": {},
   "source": [
    "### Here define the sequence length we want to consider (don't make too long ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8460d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3ac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(bptt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c70d0b",
   "metadata": {},
   "source": [
    "### Instantiate transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33996ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 1  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0 # Kinda need this to be zero :/\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67de5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('transformer_encoder.layers.0.self_attn.in_proj_weight', torch.Size([1536, 512])), ('transformer_encoder.layers.0.self_attn.in_proj_bias', torch.Size([1536])), ('transformer_encoder.layers.0.self_attn.out_proj.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.self_attn.out_proj.bias', torch.Size([512])), ('transformer_encoder.layers.0.linear1.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.linear1.bias', torch.Size([512])), ('transformer_encoder.layers.0.linear2.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.linear2.bias', torch.Size([512])), ('transformer_encoder.layers.0.norm1.weight', torch.Size([512])), ('transformer_encoder.layers.0.norm1.bias', torch.Size([512])), ('transformer_encoder.layers.0.norm2.weight', torch.Size([512])), ('transformer_encoder.layers.0.norm2.bias', torch.Size([512])), ('encoder.weight', torch.Size([28782, 512])), ('decoder.weight', torch.Size([28782, 512])), ('decoder.bias', torch.Size([28782]))]\n"
     ]
    }
   ],
   "source": [
    "print([(k, v.shape) for k,v in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ce27b",
   "metadata": {},
   "source": [
    "### Get some sample sentence just to see what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f05d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are\n",
      "arikamedu was in <unk> , in a communication from the consul of the indo @-@ french colony of pondicherry .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 100)\n",
    "print('\\n'.join([' '.join(vocab.lookup_tokens(data[:, i].tolist())) for i in range(data.shape[1])]))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e49ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114df50",
   "metadata": {},
   "source": [
    "## Attack setup starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2736043",
   "metadata": {},
   "source": [
    "### First, let's define our linear measurement - some Gaussian vector here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6db0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = torch.randn(512)\n",
    "std, mu = torch.std_mean(weights)\n",
    "measurement = (weights - mu) / std / math.sqrt(512) # Here's our linear measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c090e21",
   "metadata": {},
   "source": [
    "### Here we set up the MHA stuff for inter-sequence separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2704e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in_proj_weight', torch.Size([1536, 512])), ('in_proj_bias', torch.Size([1536])), ('out_proj.weight', torch.Size([512, 512])), ('out_proj.bias', torch.Size([512]))]\n"
     ]
    }
   ],
   "source": [
    "# Here we see about the MHA layer. in_proj_weight is Q, K, V matrices.\n",
    "print([(k, v.shape) for k,v in getattr(model.transformer_encoder.layers, '0').self_attn.named_parameters()])\n",
    "# In order: \"q, k, v\"\n",
    "\n",
    "# Let's set the query matrix to produce just the first positional encoding (or could be any index - might want last index)\n",
    "qkv_shape = getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data.shape[0] \n",
    "\n",
    "# Dummy inputs to get positional encoding\n",
    "inputs, targets = get_batch(train_data, 100)\n",
    "inputs = inputs.to(device=device)\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# Q matrix setup\n",
    "# Make the position super super large to skew softmax\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_bias.data[:qkv_shape//3] = 1000*just_pos[0,0,:]\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[:qkv_shape//3] = torch.zeros((qkv_shape//3, qkv_shape//3))\n",
    "\n",
    "# K matrix setup (identity)\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[qkv_shape//3:2*(qkv_shape//3)] = torch.eye(qkv_shape//3)\n",
    "\n",
    "# V matrix setup (identity)\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[2*(qkv_shape//3):] = torch.eye(qkv_shape//3)\n",
    "\n",
    "# Linear layer at the end of MHA - set to small value to not 'skew' embeddings too much\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data = 0.05*torch.eye(qkv_shape//3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71e41a",
   "metadata": {},
   "source": [
    "### Setting the second linear layer to just all zeros except for first row helps? Need to figure out why ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc18cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data)\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0] = torch.ones_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0])\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.bias.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bf4b3",
   "metadata": {},
   "source": [
    "### Now let's get the feature statistics for our random measurement vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14ac0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def feature_distribution(model):\n",
    "        \"\"\"Compute the mean and std of the feature layer of the given network.\"\"\"\n",
    "        features = dict()\n",
    "        setup = dict(device=device)\n",
    "        def named_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                features[name] = input[0]\n",
    "            return hook_fn\n",
    "        \n",
    "        for name, module in list(model.named_modules()):\n",
    "            if name == 'transformer_encoder.layers.0.linear1':\n",
    "                print(f'In Linear: {name}')\n",
    "                hook = module.register_forward_hook(named_hook(name))\n",
    "                feature_layer_name = name\n",
    "                break\n",
    "        feats = []\n",
    "        feats_before = []\n",
    "        model.train()\n",
    "        model.to(**setup)\n",
    "        print(f'Computing feature distribution before the {feature_layer_name} layer from external data.')\n",
    "#         for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "#             inputs, targets = get_batch(train_data, i)\n",
    "#             inputs = inputs.to(**setup)\n",
    "#             model(inputs, src_mask)\n",
    "#         model.eval()\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "            feats.append(features[feature_layer_name].detach().view(inputs.shape[0]*inputs.shape[1], -1).clone().cpu())\n",
    "        std, mu = torch.std_mean(torch.mm(torch.cat(feats), measurement.unsqueeze(1)).squeeze())\n",
    "        print(f'Feature mean is {mu.item()}, feature std is {std.item()}.')\n",
    "        model.eval()\n",
    "        model.cpu()\n",
    "        hook.remove()\n",
    "\n",
    "        return std, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a275",
   "metadata": {},
   "source": [
    "### Now we'll construct the imprint weights. Only for the first linear layer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linear: transformer_encoder.layers.0.linear1\n",
      "Computing feature distribution before the transformer_encoder.layers.0.linear1 layer from external data.\n"
     ]
    }
   ],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def _get_bins(mean, std, num_bins):\n",
    "    bins = []\n",
    "    mass_per_bin = 1 / (num_bins)\n",
    "    bins.append(-10)  # -Inf is not great here, but NormalDist(mu=0, sigma=1).cdf(10) approx 1\n",
    "    for i in range(1, num_bins):\n",
    "        bins.append(NormalDist().inv_cdf(i * mass_per_bin)*std + mean)\n",
    "    return bins\n",
    "\n",
    "def _make_biases(bias_layer, bins):\n",
    "    new_biases = torch.zeros_like(bias_layer.data)\n",
    "    for i in range(new_biases.shape[0]):\n",
    "        new_biases[i] = -bins[i]\n",
    "    return new_biases\n",
    "\n",
    "feature_std, feature_mean = feature_distribution(model)\n",
    "bins = _get_bins(feature_mean, feature_std, model.d_model)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.weight.data = measurement.repeat(model.d_model, 1)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.bias.data = _make_biases(getattr(model.transformer_encoder.layers, '0').linear1.bias, bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a51d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328e91db",
   "metadata": {},
   "source": [
    "### Get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "inputs, targets = get_batch(train_data, 100)\n",
    "#inputs, targets = get_batch(train_data, 0)\n",
    "inputs = inputs.to(device=device)\n",
    "model.to(device=device)\n",
    "model.zero_grad()\n",
    "output = model(inputs, src_mask)\n",
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08bc4e",
   "metadata": {},
   "source": [
    "## Actual attack starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868ee82",
   "metadata": {},
   "source": [
    "### Now get the bag of words, as well as the reconstructed positionally encoded stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_tokens = ((model.encoder.weight.grad != 0).sum(dim=1) > 0).nonzero().squeeze() # Bag of words tokens\n",
    "weight_grad = getattr(model.transformer_encoder.layers, '0').linear1.weight.grad.detach().clone().cpu()\n",
    "bias_grad = getattr(model.transformer_encoder.layers, '0').linear1.bias.grad.detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed positionally encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4097d2",
   "metadata": {},
   "source": [
    "### Now the messy stuff ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80945e",
   "metadata": {},
   "source": [
    "### Let's associate tokens with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33461f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the features of our bag of words sans positional encoding\n",
    "no_pos = getattr(model.transformer_encoder.layers, '0').norm1((model.encoder(leaked_tokens) * math.sqrt(model.d_model))).cpu()\n",
    "with_pos = recs # Here are those same features, but with positional encodings (stuff we reconstructed)\n",
    "\n",
    "import numpy as np\n",
    "indcs = []\n",
    "corrs = torch.zeros((len(no_pos), len(with_pos))) \n",
    "\n",
    "# We need to find out what word led to what positionally encoded representation. \n",
    "# Let's try the naive greedy search for correlations between no_pos and with_pos as defined above\n",
    "for i, no_p in enumerate(no_pos):\n",
    "    max_corr = 0\n",
    "    for j, with_p in enumerate(with_pos):\n",
    "        val = np.corrcoef(np.array([no_p.detach().numpy(), with_p.detach().numpy()]))[0,1]\n",
    "        corrs[i,j] = val\n",
    "\n",
    "# Find which positionally-encoded vector associates with un-positionally-encoded vector\n",
    "from scipy.optimize import linear_sum_assignment # Better than greedy search? \n",
    "row_ind, col_ind = linear_sum_assignment(corrs.numpy(), maximize=True)\n",
    "\n",
    "order = [(row_i, col_i) for (row_i, col_i) in zip(row_ind, col_ind)]\n",
    "order = sorted(order, key=lambda x: x[1])\n",
    "\n",
    "# Now let's re-sort the tokens by this order\n",
    "sorted_tokens1 = sorted_tokens1 = [leaked_tokens[order[i][0]] for i in range(len(order))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pos.shape, with_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa016b",
   "metadata": {},
   "source": [
    "### Now let's get each token's position, as well as splitting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've 'lined-up' the pos-encoded features with non-pos-encoded features, let's subtract the two\n",
    "# to get some 'faux' positions (layer norm means they aren't exact).\n",
    "estimated_pos = torch.stack([with_pos[order[i][1]] - no_pos[order[i][0]] for i in range(len(order))])\n",
    "new_with_pos = [with_pos[order[i][1]] for i in range(len(order))]\n",
    "\n",
    "# Now let's get just the additive part of the positional encoding\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# The old way of without sequence splitting... If we only have 1 user, with 1 sequence per batch, it works well. \n",
    "# Here we just get 'jumbled' sentences if there are multiple users\n",
    "\n",
    "new_just_pos = just_pos[:,0,:] # just save this thing for later :) \n",
    "just_pos = just_pos.view(-1, just_pos.shape[-1])\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "\n",
    "# We'll do another linear sum assignment, but now it's on the positions of the tokens\n",
    "# First calculate the correlations between (faux) estimated_pos and the (real) just_pos terms\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "row_ind, col_ind = linear_sum_assignment(order_coeffs.numpy(), maximize=True)\n",
    "pos_order = [(row_i, col_i) for (row_i, col_i) in zip(row_ind, col_ind)]\n",
    "pos_order = sorted(pos_order, key=lambda x: x[1])\n",
    "\n",
    "just_pos = new_just_pos\n",
    "\n",
    "# ----------- NEW STUFF -----------  Getting multiple user's sentences back\n",
    "\n",
    "# Let's calculate this matrix again, but for the new method (previous calculation was just for old method, can ignore)\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "\n",
    "# Now, we make a dictionary where keys are positions, and values are encoded embeddings. \n",
    "# i.e. word_groups[0] = ['0th_word_of_sequence1', '0th_word_of_sequence2', ...]\n",
    "from collections import defaultdict\n",
    "word_groups = defaultdict(list)\n",
    "\n",
    "for i in range(order_coeffs.shape[0]):\n",
    "    max_corr = torch.argmax(order_coeffs[i]).item()\n",
    "    word_groups[max_corr].append(i)\n",
    "\n",
    "# Sort these word groups to start forming sentences\n",
    "sorted_keys = sorted([k for k in word_groups.keys()])\n",
    "word_groups = [word_groups[k] for k in sorted_keys]\n",
    "first_words = word_groups[0]\n",
    "sentences = [[] for i in range(inputs.shape[1])] # Cheating a bit because we shouldn't know how many users there are.\n",
    "\n",
    "# Start the sentences with first words\n",
    "for i, first_w in enumerate(first_words):\n",
    "    sentences[i].append(sorted_tokens1[first_w])\n",
    "\n",
    "# Go through the rest of the word groups, assigning words to their appropriate sentences\n",
    "for w in word_groups[1:]:\n",
    "    corr = torch.zeros(len(w), len(first_words))\n",
    "    for i, x in enumerate(w):\n",
    "        for j, y in enumerate(first_words):\n",
    "            corr[i,j] = np.corrcoef(estimated_pos[x].detach().numpy(), new_with_pos[y].detach().numpy())[0,1]\n",
    "    \n",
    "    # Below we do linear sum assignment for each word to each potential sentence\n",
    "    row_ind, col_ind = linear_sum_assignment(corr.numpy(), maximize=True)\n",
    "    for m, n in zip(row_ind, col_ind):\n",
    "        sentences[n].append(sorted_tokens1[w[m]])\n",
    "\n",
    "# Here are the (old) sorted tokens - i.e. just jumbled sequences\n",
    "final_sorted_tokens = [sorted_tokens1[pos_order[i][0]] for i in range(len(pos_order))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8f558",
   "metadata": {},
   "source": [
    "### Let's see how we did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH SEQUENCES \\n\" + '-'*50)\n",
    "for i in range(inputs.shape[-1]):\n",
    "    print(f'USER SEQUENCE {i}')\n",
    "    print(' '.join(vocab.lookup_tokens([x for x in inputs[:,i]]))) # The true sentence\n",
    "    print('\\n')\n",
    "    \n",
    "print('\\n'*2)\n",
    "print(\"RECONSTRUCTED JUMBLED SEQUENCES (OLD METHOD) \\n\" + '-'*50)\n",
    "print(' '.join(vocab.lookup_tokens(final_sorted_tokens))) # What we reconstruct\n",
    "\n",
    "print('\\n'*2)\n",
    "print(\"RECONSTRUCTED SPLIT SEQUENCES (NEW METHOD) \\n\" + '-'*50)\n",
    "for i in range(len(sentences)):\n",
    "    print(f'RECONSTRUCTED SEQUENCE {i}')\n",
    "    print(' '.join(vocab.lookup_tokens(sentences[i]))) # What we reconstruct\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer_encoder.layers[0].linear1.weight.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
