{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b546f8c",
   "metadata": {},
   "source": [
    "# Parameter attacks on Transformers? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99327874",
   "metadata": {},
   "source": [
    "Following the tutorial at https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a7090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083de34",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4fef",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af97df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a979e4",
   "metadata": {},
   "source": [
    "### Data stuff (boring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ba4833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.48M/4.48M [00:00<00:00, 12.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip downloaded.\n",
      "Validating hash 542ccefacc6c27f945fb54453812b3cd matches hash of /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip\n",
      "Opening zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "Creating train data\n",
      "File /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip already exists.\n",
      "Validating hash 542ccefacc6c27f945fb54453812b3cd matches hash of /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip\n",
      "Opening zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/ already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.test.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.valid.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.train.tokens already extracted.\n",
      "Finished extracting zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "Creating train data\n",
      "File /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip already exists.\n",
      "Validating hash 542ccefacc6c27f945fb54453812b3cd matches hash of /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip\n",
      "Opening zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/ already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.test.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.valid.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.train.tokens already extracted.\n",
      "Finished extracting zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "Creating valid data\n",
      "File /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip already exists.\n",
      "Validating hash 542ccefacc6c27f945fb54453812b3cd matches hash of /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip\n",
      "Opening zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/ already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.test.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.valid.tokens already extracted.\n",
      "/nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2/wiki.train.tokens already extracted.\n",
      "Finished extracting zip file /nfshomes/lfowl/.torchtext/cache/WikiText2/wikitext-2-v1.zip.\n",
      "Creating test data\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 2\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07e93f",
   "metadata": {},
   "source": [
    "### Here define the sequence length we want to consider (don't make too long ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8460d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3ac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(bptt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c70d0b",
   "metadata": {},
   "source": [
    "### Instantiate transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33996ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 1  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0 # Kinda need this to be zero :/\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67de5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('transformer_encoder.layers.0.self_attn.in_proj_weight', torch.Size([1536, 512])), ('transformer_encoder.layers.0.self_attn.in_proj_bias', torch.Size([1536])), ('transformer_encoder.layers.0.self_attn.out_proj.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.self_attn.out_proj.bias', torch.Size([512])), ('transformer_encoder.layers.0.linear1.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.linear1.bias', torch.Size([512])), ('transformer_encoder.layers.0.linear2.weight', torch.Size([512, 512])), ('transformer_encoder.layers.0.linear2.bias', torch.Size([512])), ('transformer_encoder.layers.0.norm1.weight', torch.Size([512])), ('transformer_encoder.layers.0.norm1.bias', torch.Size([512])), ('transformer_encoder.layers.0.norm2.weight', torch.Size([512])), ('transformer_encoder.layers.0.norm2.bias', torch.Size([512])), ('encoder.weight', torch.Size([28782, 512])), ('decoder.weight', torch.Size([28782, 512])), ('decoder.bias', torch.Size([28782]))]\n"
     ]
    }
   ],
   "source": [
    "print([(k, v.shape) for k,v in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ce27b",
   "metadata": {},
   "source": [
    "### Get some sample sentence just to see what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit\n",
      "arikamedu was in <unk> , in a communication from the consul of the indo @-@ french colony of pondicherry . it informed the french east\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 100)\n",
    "data.shape\n",
    "print('\\n'.join([' '.join(vocab.lookup_tokens(data[:, i].tolist())) for i in range(data.shape[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114df50",
   "metadata": {},
   "source": [
    "## Attack setup starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2736043",
   "metadata": {},
   "source": [
    "### First, let's define our linear measurement - some Gaussian vector here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6db0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = torch.randn(512)\n",
    "std, mu = torch.std_mean(weights)\n",
    "measurement = (weights - mu) / std / math.sqrt(512) # Here's our linear measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c090e21",
   "metadata": {},
   "source": [
    "### Here we set up the MHA stuff for inter-sequence separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2704e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in_proj_weight', torch.Size([1536, 512])), ('in_proj_bias', torch.Size([1536])), ('out_proj.weight', torch.Size([512, 512])), ('out_proj.bias', torch.Size([512]))]\n",
      "torch.Size([25, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10427/1504598974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mjust_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Q matrix setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/lfowl/miniconda3/envs/breaching/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/lfowl/miniconda3/envs/breaching/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/cmlscratch/lfowl/miniconda3/envs/breaching/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Here we see about the MHA layer. in_proj_weight is Q, K, V matrices.\n",
    "print([(k, v.shape) for k,v in getattr(model.transformer_encoder.layers, '0').self_attn.named_parameters()])\n",
    "# In order: \"q, k, v\"\n",
    "\n",
    "sequence_token_weight = 0.075 \n",
    "\n",
    "# Let's set the query matrix to produce just the first positional encoding (or could be any index - might want last index)\n",
    "qkv_shape = getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data.shape[0] \n",
    "\n",
    "# Dummy inputs to get positional encoding\n",
    "inputs, targets = get_batch(train_data, 100)\n",
    "inputs = inputs.to(device=device)\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# Q matrix setup\n",
    "# We make the weight 0, and the bias some (large multiple of) positional encoding\n",
    "# Make the position super super large to skew softmax\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_bias.data[:qkv_shape//3] = 1000*just_pos[0,0,:]\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[:qkv_shape//3] = torch.zeros((qkv_shape//3, qkv_shape//3))\n",
    "\n",
    "# K matrix setup (identity)\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[qkv_shape//3:2*(qkv_shape//3)] = torch.eye(qkv_shape//3)\n",
    "\n",
    "# V matrix setup (identity)\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.in_proj_weight.data[2*(qkv_shape//3):] = torch.eye(qkv_shape//3)\n",
    "\n",
    "# So, (QK^T)V just adds the same vector (first word embedding) to each word in the sequence.  \n",
    "\n",
    "# Linear layer at the end of MHA - set to small value to not 'skew' embeddings too much\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data = sequence_token_weight*torch.eye(qkv_shape//3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406b9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71e41a",
   "metadata": {},
   "source": [
    "### Setting the second linear layer to just all zeros except for first row helps? Need to figure out why ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dc18cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data)\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0] = torch.ones_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0])\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.bias.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bf4b3",
   "metadata": {},
   "source": [
    "### Now let's get the feature statistics for our random measurement vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "14ac0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_distribution(model):\n",
    "        \"\"\"Compute the mean and std of the feature layer of the given network.\"\"\"\n",
    "        features = dict()\n",
    "        setup = dict(device=device)\n",
    "        def named_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                features[name] = input[0]\n",
    "            return hook_fn\n",
    "        \n",
    "        for name, module in list(model.named_modules()):\n",
    "            if name == 'transformer_encoder.layers.0.linear1':\n",
    "                print(f'In Linear: {name}')\n",
    "                hook = module.register_forward_hook(named_hook(name))\n",
    "                feature_layer_name = name\n",
    "                break\n",
    "        feats = []\n",
    "        feats_before = []\n",
    "        model.train()\n",
    "        model.to(**setup)\n",
    "        print(f'Computing feature distribution before the {feature_layer_name} layer from external data.')\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "        model.eval()\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "            feats.append(features[feature_layer_name].detach().view(inputs.shape[0]*inputs.shape[1], -1).clone().cpu())\n",
    "        std, mu = torch.std_mean(torch.mm(torch.cat(feats), measurement.unsqueeze(1)).squeeze())\n",
    "        print(f'Feature mean is {mu.item()}, feature std is {std.item()}.')\n",
    "        model.eval()\n",
    "        model.cpu()\n",
    "        hook.remove()\n",
    "\n",
    "        return std, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a275",
   "metadata": {},
   "source": [
    "### Now we'll construct the imprint weights. Only for the first linear layer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e6b66f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linear: transformer_encoder.layers.0.linear1\n",
      "Computing feature distribution before the transformer_encoder.layers.0.linear1 layer from external data.\n",
      "Feature mean is -0.11086225509643555, feature std is 0.8909378051757812.\n"
     ]
    }
   ],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def _get_bins(mean, std, num_bins):\n",
    "    bins = []\n",
    "    mass_per_bin = 1 / (num_bins)\n",
    "    bins.append(-10)  # -Inf is not great here, but NormalDist(mu=0, sigma=1).cdf(10) approx 1\n",
    "    for i in range(1, num_bins):\n",
    "        bins.append(NormalDist().inv_cdf(i * mass_per_bin)*std + mean)\n",
    "    return bins\n",
    "\n",
    "def _make_biases(bias_layer, bins):\n",
    "    new_biases = torch.zeros_like(bias_layer.data)\n",
    "    for i in range(new_biases.shape[0]):\n",
    "        new_biases[i] = -bins[i]\n",
    "    return new_biases\n",
    "\n",
    "feature_std, feature_mean = feature_distribution(model)\n",
    "bins = _get_bins(feature_mean, feature_std, model.d_model)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.weight.data = measurement.repeat(model.d_model, 1)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.bias.data = _make_biases(getattr(model.transformer_encoder.layers, '0').linear1.bias, bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e91db",
   "metadata": {},
   "source": [
    "### Get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e5ba806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "inputs, targets = get_batch(train_data, 1000)\n",
    "#inputs, targets = get_batch(train_data, 0)\n",
    "inputs = inputs.to(device=device)\n",
    "model.to(device=device)\n",
    "model.zero_grad()\n",
    "output = model(inputs, src_mask)\n",
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08bc4e",
   "metadata": {},
   "source": [
    "## Actual attack starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868ee82",
   "metadata": {},
   "source": [
    "### Now get the bag of words, as well as the reconstructed positionally encoded stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "778d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_tokens = ((model.encoder.weight.grad != 0).sum(dim=1) > 0).nonzero().squeeze() # Bag of words tokens\n",
    "weight_grad = getattr(model.transformer_encoder.layers, '0').linear1.weight.grad.detach().clone().cpu()\n",
    "bias_grad = getattr(model.transformer_encoder.layers, '0').linear1.bias.grad.detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed positionally encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4097d2",
   "metadata": {},
   "source": [
    "### Now the messy stuff ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89ac40",
   "metadata": {},
   "source": [
    "### Let's associate tokens with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e566a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the features of our bag of words sans positional encoding\n",
    "no_pos = getattr(model.transformer_encoder.layers, '0').norm1((model.encoder(leaked_tokens) * math.sqrt(model.d_model))).cpu()\n",
    "with_pos = recs # Here are those same features, but with positional encodings (stuff we reconstructed)\n",
    "\n",
    "import numpy as np\n",
    "indcs = []\n",
    "corrs = torch.zeros((len(no_pos), len(with_pos))) \n",
    "\n",
    "# We need to find out what word led to what positionally encoded representation. \n",
    "# Let's try the naive greedy search for correlations between no_pos and with_pos as defined above\n",
    "for i, no_p in enumerate(no_pos):\n",
    "    max_corr = 0\n",
    "    for j, with_p in enumerate(with_pos):\n",
    "        val = np.corrcoef(np.array([no_p.detach().numpy(), with_p.detach().numpy()]))[0,1]\n",
    "        corrs[i,j] = val\n",
    "\n",
    "# Find which positionally-encoded vector associates with un-positionally-encoded vector\n",
    "from scipy.optimize import linear_sum_assignment # Better than greedy search? \n",
    "row_ind, col_ind = linear_sum_assignment(corrs.numpy(), maximize=True)\n",
    "\n",
    "order = [(row_i, col_i) for (row_i, col_i) in zip(row_ind, col_ind)]\n",
    "order = sorted(order, key=lambda x: x[1])\n",
    "\n",
    "# Now let's re-sort the tokens by this order\n",
    "sorted_tokens1 = [leaked_tokens[order[i][0]] for i in range(len(order))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd169d",
   "metadata": {},
   "source": [
    "### Now let's get each token's position, as well as splitting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "66ee6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've 'lined-up' the pos-encoded features with non-pos-encoded features, let's subtract the two\n",
    "# to get some 'faux' positions (layer norm means they aren't exact).\n",
    "estimated_pos = torch.stack([with_pos[order[i][1]] - no_pos[order[i][0]] for i in range(len(order))])\n",
    "new_with_pos = [with_pos[order[i][1]] for i in range(len(order))]\n",
    "\n",
    "# Now let's get just the additive part of the positional encoding\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# The old way of without sequence splitting... If we only have 1 user, with 1 sequence per batch, it works well. \n",
    "# Here we just get 'jumbled' sentences if there are multiple users\n",
    "\n",
    "new_just_pos = just_pos[:,0,:] # just save this thing for later :) \n",
    "just_pos = just_pos.view(-1, just_pos.shape[-1])\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "\n",
    "# We'll do another linear sum assignment, but now it's on the positions of the tokens\n",
    "# First calculate the correlations between (faux) estimated_pos and the (real) just_pos terms\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "row_ind, col_ind = linear_sum_assignment(order_coeffs.numpy(), maximize=True)\n",
    "pos_order = [(row_i, col_i) for (row_i, col_i) in zip(row_ind, col_ind)]\n",
    "pos_order = sorted(pos_order, key=lambda x: x[1])\n",
    "\n",
    "just_pos = new_just_pos\n",
    "\n",
    "# ----------- NEW STUFF -----------  Getting multiple user's sentences back\n",
    "\n",
    "# Let's calculate this matrix again, but for the new method (previous calculation was just for old method, can ignore)\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "\n",
    "# Now, we make a dictionary where keys are positions, and values are encoded embeddings. \n",
    "# i.e. word_groups[0] = ['0th_word_of_sequence1', '0th_word_of_sequence2', ...]\n",
    "from collections import defaultdict\n",
    "word_groups = defaultdict(list)\n",
    "\n",
    "for i in range(order_coeffs.shape[0]):\n",
    "    max_corr = torch.argmax(order_coeffs[i]).item()\n",
    "    word_groups[max_corr].append(i)\n",
    "\n",
    "# Sort these word groups to start forming sentences\n",
    "sorted_keys = sorted([k for k in word_groups.keys()])\n",
    "word_groups = [word_groups[k] for k in sorted_keys]\n",
    "first_words = word_groups[0]\n",
    "sentences = [[] for i in range(inputs.shape[1])] # Cheating a bit because we shouldn't know how many users there are.\n",
    "\n",
    "# Start the sentences with first words\n",
    "for i, first_w in enumerate(first_words):\n",
    "    sentences[i].append(sorted_tokens1[first_w])\n",
    "\n",
    "# Go through the rest of the word groups, assigning words to their appropriate sentences\n",
    "for w in word_groups[1:]:\n",
    "    corr = torch.zeros(len(w), len(first_words))\n",
    "    for i, x in enumerate(w):\n",
    "        for j, y in enumerate(first_words):\n",
    "            corr[i,j] = np.corrcoef(estimated_pos[x].detach().numpy(), new_with_pos[y].detach().numpy())[0,1]\n",
    "    \n",
    "    # Below we do linear sum assignment for each word to each potential sentence\n",
    "    row_ind, col_ind = linear_sum_assignment(corr.numpy(), maximize=True)\n",
    "    for m, n in zip(row_ind, col_ind):\n",
    "        sentences[n].append(sorted_tokens1[w[m]])\n",
    "\n",
    "# Here are the (old) sorted tokens - i.e. just jumbled sequences\n",
    "final_sorted_tokens = [sorted_tokens1[pos_order[i][0]] for i in range(len(pos_order))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8f558",
   "metadata": {},
   "source": [
    "### Let's see how we did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "065ccaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH SEQUENCES \n",
      "--------------------------------------------------\n",
      "USER SEQUENCE 0\n",
      "<unk> , meaning always ready . the three main characters are <unk> kurt irving , an army officer falsely accused of treason who wishes to\n",
      "\n",
      "\n",
      "USER SEQUENCE 1\n",
      "she started researching on the beads , organized a proper <unk> display of the artifacts of the site at the pondicherry museum , and brought\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RECONSTRUCTED JUMBLED SEQUENCES (OLD METHOD) \n",
      "--------------------------------------------------\n",
      "<unk> she , started meaning researching always on ready the beads . army organized three a proper characters are display of kurt at artifacts irving main an site officer falsely accused pondicherry treason museum who and wishes brought to\n",
      "\n",
      "\n",
      "\n",
      "RECONSTRUCTED SPLIT SEQUENCES (NEW METHOD) \n",
      "--------------------------------------------------\n",
      "RECONSTRUCTED SEQUENCE 0\n",
      "<unk> , meaning always ready . army three characters are kurt irving an officer falsely accused treason who wishes to\n",
      "\n",
      "\n",
      "RECONSTRUCTED SEQUENCE 1\n",
      "she started researching on the beads organized a proper display of at artifacts main site pondicherry museum and brought\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH SEQUENCES \\n\" + '-'*50)\n",
    "for i in range(inputs.shape[-1]):\n",
    "    print(f'USER SEQUENCE {i}')\n",
    "    print(' '.join(vocab.lookup_tokens([x for x in inputs[:,i]]))) # The true sentence\n",
    "    print('\\n')\n",
    "    \n",
    "print('\\n'*2)\n",
    "print(\"RECONSTRUCTED JUMBLED SEQUENCES (OLD METHOD) \\n\" + '-'*50)\n",
    "print(' '.join(vocab.lookup_tokens(final_sorted_tokens))) # What we reconstruct\n",
    "\n",
    "print('\\n'*2)\n",
    "print(\"RECONSTRUCTED SPLIT SEQUENCES (NEW METHOD) \\n\" + '-'*50)\n",
    "for i in range(len(sentences)):\n",
    "    print(f'RECONSTRUCTED SEQUENCE {i}')\n",
    "    print(' '.join(vocab.lookup_tokens(sentences[i]))) # What we reconstruct\n",
    "    print('\\n')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
