{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b546f8c",
   "metadata": {},
   "source": [
    "# Parameter attacks on Transformers? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99327874",
   "metadata": {},
   "source": [
    "Following the tutorial at https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a7090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083de34",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4fef",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af97df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a979e4",
   "metadata": {},
   "source": [
    "### Data stuff (boring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ba4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 1\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07e93f",
   "metadata": {},
   "source": [
    "### Here define the sequence length we want to consider (don't make too long ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8460d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3ac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(bptt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c70d0b",
   "metadata": {},
   "source": [
    "### Instantiate transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33996ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0 # Kinda need this to be zero :/\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ce27b",
   "metadata": {},
   "source": [
    "### Get some sample sentence just to see what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit <unk> raven . the game\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 100)\n",
    "data.shape\n",
    "print('\\n'.join([' '.join(vocab.lookup_tokens(data[:, i].tolist())) for i in range(data.shape[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114df50",
   "metadata": {},
   "source": [
    "## Attack setup starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2736043",
   "metadata": {},
   "source": [
    "### First, let's define our linear measurement - some Gaussian vector here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6db0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = torch.randn(200)\n",
    "std, mu = torch.std_mean(weights)\n",
    "measurement = (weights - mu) / std / math.sqrt(200) # Here's our linear measurement\n",
    "getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').self_attn.out_proj.weight.data)\n",
    "# Above we set the attention head to be zero. Probably should use these somehow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71e41a",
   "metadata": {},
   "source": [
    "### Setting the second linear layer to just all zeros except for first row helps? Need to figure out why ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc18cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data)\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0] = torch.ones_like(getattr(model.transformer_encoder.layers, '0').linear2.weight.data[0])\n",
    "getattr(model.transformer_encoder.layers, '0').linear2.bias.data = torch.zeros_like(getattr(model.transformer_encoder.layers, '0').linear2.bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bf4b3",
   "metadata": {},
   "source": [
    "### Now let's get the feature statistics for our random measurement vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ac0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_distribution(model):\n",
    "        \"\"\"Compute the mean and std of the feature layer of the given network.\"\"\"\n",
    "        features = dict()\n",
    "        setup = dict(device=device)\n",
    "        def named_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                features[name] = input[0]\n",
    "            return hook_fn\n",
    "        \n",
    "        for name, module in list(model.named_modules()):\n",
    "            if name == 'transformer_encoder.layers.0.linear1':\n",
    "                print(f'In Linear: {name}')\n",
    "                hook = module.register_forward_hook(named_hook(name))\n",
    "                feature_layer_name = name\n",
    "                break\n",
    "        feats = []\n",
    "        feats_before = []\n",
    "        model.train()\n",
    "        model.to(**setup)\n",
    "        print(f'Computing feature distribution before the {feature_layer_name} layer from external data.')\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "        model.eval()\n",
    "        for batch, i in enumerate(list(range(0, train_data.size(0) - 1, bptt))[:-1]):\n",
    "            inputs, targets = get_batch(train_data, i)\n",
    "            inputs = inputs.to(**setup)\n",
    "            model(inputs, src_mask)\n",
    "            feats.append(features[feature_layer_name].detach().view(inputs.shape[0]*inputs.shape[1], -1).clone().cpu())\n",
    "        std, mu = torch.std_mean(torch.mm(torch.cat(feats), measurement.unsqueeze(1)).squeeze())\n",
    "        print(f'Feature mean is {mu.item()}, feature std is {std.item()}.')\n",
    "        model.eval()\n",
    "        model.cpu()\n",
    "        hook.remove()\n",
    "\n",
    "        return std, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a275",
   "metadata": {},
   "source": [
    "### Now we'll construct the imprint weights. Only for the first linear layer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b66f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linear: transformer_encoder.layers.0.linear1\n",
      "Computing feature distribution before the transformer_encoder.layers.0.linear1 layer from external data.\n",
      "Feature mean is -0.16924509406089783, feature std is 0.9067009687423706.\n"
     ]
    }
   ],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def _get_bins(mean, std, num_bins):\n",
    "    bins = []\n",
    "    mass_per_bin = 1 / (num_bins)\n",
    "    bins.append(-10)  # -Inf is not great here, but NormalDist(mu=0, sigma=1).cdf(10) approx 1\n",
    "    for i in range(1, num_bins):\n",
    "        bins.append(NormalDist().inv_cdf(i * mass_per_bin)*std + mean)\n",
    "    return bins\n",
    "\n",
    "def _make_biases(bias_layer, bins):\n",
    "    new_biases = torch.zeros_like(bias_layer.data)\n",
    "    for i in range(new_biases.shape[0]):\n",
    "        new_biases[i] = -bins[i]\n",
    "    return new_biases\n",
    "\n",
    "feature_std, feature_mean = feature_distribution(model)\n",
    "bins = _get_bins(feature_mean, feature_std, model.d_model)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.weight.data = measurement.repeat(model.d_model, 1)\n",
    "getattr(model.transformer_encoder.layers, '0').linear1.bias.data = _make_biases(getattr(model.transformer_encoder.layers, '0').linear1.bias, bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e91db",
   "metadata": {},
   "source": [
    "### Get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ba806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "inputs, targets = get_batch(train_data, 100)\n",
    "#inputs, targets = get_batch(train_data, 0)\n",
    "inputs = inputs.to(device=device)\n",
    "model.to(device=device)\n",
    "model.zero_grad()\n",
    "output = model(inputs, src_mask)\n",
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08bc4e",
   "metadata": {},
   "source": [
    "## Actual attack starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868ee82",
   "metadata": {},
   "source": [
    "### Now get the bag of words, as well as the reconstructed positionally encoded stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "778d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_tokens = ((model.encoder.weight.grad != 0).sum(dim=1) > 0).nonzero().squeeze() # Bag of words tokens\n",
    "weight_grad = getattr(model.transformer_encoder.layers, '0').linear1.weight.grad.detach().clone().cpu()\n",
    "bias_grad = getattr(model.transformer_encoder.layers, '0').linear1.bias.grad.detach().clone().cpu()\n",
    "\n",
    "for i in reversed(list(range(1, weight_grad.shape[0]))):\n",
    "    weight_grad[i] -= weight_grad[i - 1]\n",
    "    bias_grad[i] -= bias_grad[i - 1]\n",
    "valid_classes = bias_grad != 0\n",
    "\n",
    "recs = weight_grad[valid_classes, :] / bias_grad[valid_classes, None] # Here are our reconstructed positionally encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4097d2",
   "metadata": {},
   "source": [
    "### Now the messy stuff ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ee6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9086, 0.9654, 0.9091, 0.7721, 0.6410, 0.5520, 0.5071, 0.4940, 0.4929,\n",
      "         0.4865, 0.4701, 0.4492, 0.4303, 0.4160, 0.4043, 0.3918, 0.3779, 0.3652,\n",
      "         0.3564, 0.3521, 0.3503, 0.3484, 0.3452, 0.3420, 0.3403, 0.3399, 0.3389,\n",
      "         0.3355, 0.3299, 0.3245],\n",
      "        [0.3604, 0.3528, 0.3534, 0.3540, 0.3581, 0.3718, 0.4013, 0.4464, 0.4928,\n",
      "         0.5210, 0.5275, 0.5319, 0.5643, 0.6433, 0.7567, 0.8608, 0.9088, 0.8820,\n",
      "         0.7980, 0.6971, 0.6189, 0.5804, 0.5704, 0.5652, 0.5491, 0.5235, 0.5001,\n",
      "         0.4880, 0.4853, 0.4829],\n",
      "        [0.3389, 0.3309, 0.3631, 0.4120, 0.4501, 0.4657, 0.4669, 0.4747, 0.5034,\n",
      "         0.5456, 0.5805, 0.5966, 0.6059, 0.6377, 0.7139, 0.8236, 0.9224, 0.9618,\n",
      "         0.9205, 0.8177, 0.7020, 0.6214, 0.5918, 0.5934, 0.5936, 0.5742, 0.5415,\n",
      "         0.5149, 0.5065, 0.5102],\n",
      "        [0.3116, 0.3121, 0.3217, 0.3356, 0.3557, 0.3800, 0.4042, 0.4247, 0.4393,\n",
      "         0.4505, 0.4638, 0.4809, 0.4964, 0.5014, 0.4920, 0.4764, 0.4720, 0.4916,\n",
      "         0.5321, 0.5751, 0.5999, 0.6020, 0.6017, 0.6300, 0.7047, 0.8122, 0.9098,\n",
      "         0.9509, 0.9166, 0.8276],\n",
      "        [0.3687, 0.3515, 0.3579, 0.3871, 0.4255, 0.4547, 0.4654, 0.4632, 0.4649,\n",
      "         0.4848, 0.5213, 0.5583, 0.5798, 0.5854, 0.5949, 0.6364, 0.7220, 0.8335,\n",
      "         0.9294, 0.9673, 0.9294, 0.8363, 0.7346, 0.6646, 0.6372, 0.6349, 0.6300,\n",
      "         0.6060, 0.5677, 0.5327],\n",
      "        [0.3068, 0.2978, 0.3160, 0.3421, 0.3608, 0.3687, 0.3735, 0.3862, 0.4096,\n",
      "         0.4339, 0.4479, 0.4501, 0.4495, 0.4568, 0.4742, 0.4922, 0.4999, 0.4968,\n",
      "         0.4946, 0.5075, 0.5386, 0.5751, 0.6000, 0.6106, 0.6244, 0.6679, 0.7527,\n",
      "         0.8590, 0.9432, 0.9659],\n",
      "        [0.3130, 0.2989, 0.3202, 0.3594, 0.3940, 0.4131, 0.4197, 0.4262, 0.4414,\n",
      "         0.4628, 0.4811, 0.4914, 0.4985, 0.5116, 0.5344, 0.5583, 0.5711, 0.5714,\n",
      "         0.5760, 0.6113, 0.6922, 0.8031, 0.9009, 0.9413, 0.9057, 0.8121, 0.7051,\n",
      "         0.6285, 0.5990, 0.6009],\n",
      "        [0.3729, 0.3823, 0.4015, 0.4204, 0.4437, 0.4740, 0.5082, 0.5382, 0.5548,\n",
      "         0.5600, 0.5732, 0.6184, 0.7053, 0.8173, 0.9124, 0.9463, 0.9037, 0.8069,\n",
      "         0.6975, 0.6129, 0.5682, 0.5536, 0.5470, 0.5324, 0.5088, 0.4858, 0.4723,\n",
      "         0.4689, 0.4695, 0.4675],\n",
      "        [0.5069, 0.5349, 0.6462, 0.7955, 0.9188, 0.9671, 0.9202, 0.7986, 0.6586,\n",
      "         0.5610, 0.5283, 0.5370, 0.5476, 0.5360, 0.5051, 0.4747, 0.4602, 0.4586,\n",
      "         0.4553, 0.4399, 0.4159, 0.3966, 0.3917, 0.3978, 0.4026, 0.3961, 0.3794,\n",
      "         0.3630, 0.3566, 0.3602],\n",
      "        [0.6283, 0.7662, 0.9133, 0.9678, 0.9183, 0.7977, 0.6547, 0.5432, 0.4958,\n",
      "         0.5012, 0.5169, 0.5077, 0.4691, 0.4228, 0.3952, 0.3941, 0.4053, 0.4083,\n",
      "         0.3957, 0.3767, 0.3663, 0.3693, 0.3764, 0.3740, 0.3580, 0.3379, 0.3281,\n",
      "         0.3346, 0.3492, 0.3572],\n",
      "        [0.3237, 0.3267, 0.3435, 0.3660, 0.3906, 0.4105, 0.4194, 0.4182, 0.4152,\n",
      "         0.4229, 0.4481, 0.4852, 0.5197, 0.5392, 0.5451, 0.5560, 0.5968, 0.6782,\n",
      "         0.7843, 0.8769, 0.9139, 0.8766, 0.7840, 0.6793, 0.6021, 0.5685, 0.5678,\n",
      "         0.5746, 0.5681, 0.5444],\n",
      "        [0.3588, 0.3316, 0.3171, 0.3184, 0.3300, 0.3436, 0.3580, 0.3808, 0.4193,\n",
      "         0.4718, 0.5268, 0.5692, 0.5873, 0.5777, 0.5488, 0.5196, 0.5119, 0.5394,\n",
      "         0.6012, 0.6809, 0.7500, 0.7790, 0.7533, 0.6807, 0.5862, 0.5008, 0.4485,\n",
      "         0.4356, 0.4489, 0.4663],\n",
      "        [0.3133, 0.2930, 0.2946, 0.3205, 0.3608, 0.3991, 0.4235, 0.4323, 0.4341,\n",
      "         0.4415, 0.4605, 0.4853, 0.5038, 0.5085, 0.5042, 0.5054, 0.5239, 0.5575,\n",
      "         0.5908, 0.6077, 0.6069, 0.6074, 0.6366, 0.7087, 0.8097, 0.9020, 0.9437,\n",
      "         0.9147, 0.8305, 0.7299],\n",
      "        [0.3266, 0.2931, 0.2822, 0.2999, 0.3318, 0.3580, 0.3688, 0.3699, 0.3755,\n",
      "         0.3956, 0.4265, 0.4549, 0.4708, 0.4763, 0.4845, 0.5073, 0.5426, 0.5748,\n",
      "         0.5886, 0.5852, 0.5873, 0.6250, 0.7105, 0.8234, 0.9189, 0.9529, 0.9090,\n",
      "         0.8109, 0.7071, 0.6377],\n",
      "        [0.3953, 0.3950, 0.4005, 0.4195, 0.4601, 0.5130, 0.5577, 0.5760, 0.5672,\n",
      "         0.5575, 0.5849, 0.6679, 0.7887, 0.8990, 0.9453, 0.9051, 0.8051, 0.6994,\n",
      "         0.6310, 0.6098, 0.6146, 0.6142, 0.5930, 0.5594, 0.5332, 0.5265, 0.5341,\n",
      "         0.5394, 0.5294, 0.5061],\n",
      "        [0.4698, 0.4656, 0.4826, 0.5389, 0.6399, 0.7648, 0.8760, 0.9311, 0.9030,\n",
      "         0.8052, 0.6874, 0.5975, 0.5542, 0.5476, 0.5518, 0.5449, 0.5232, 0.4987,\n",
      "         0.4844, 0.4833, 0.4870, 0.4839, 0.4697, 0.4499, 0.4340, 0.4266, 0.4244,\n",
      "         0.4195, 0.4074, 0.3906],\n",
      "        [0.4103, 0.3953, 0.4084, 0.4507, 0.5045, 0.5439, 0.5539, 0.5441, 0.5469,\n",
      "         0.5966, 0.6993, 0.8250, 0.9237, 0.9523, 0.8987, 0.7924, 0.6851, 0.6149,\n",
      "         0.5876, 0.5836, 0.5782, 0.5587, 0.5302, 0.5052, 0.4909, 0.4847, 0.4790,\n",
      "         0.4686, 0.4543, 0.4400],\n",
      "        [0.4044, 0.3991, 0.4265, 0.4749, 0.5221, 0.5496, 0.5568, 0.5657, 0.6076,\n",
      "         0.6962, 0.8110, 0.9085, 0.9468, 0.9085, 0.8123, 0.7045, 0.6289, 0.5988,\n",
      "         0.5967, 0.5946, 0.5751, 0.5415, 0.5102, 0.4938, 0.4906, 0.4885, 0.4765,\n",
      "         0.4540, 0.4308, 0.4175],\n",
      "        [0.3764, 0.3465, 0.3488, 0.3843, 0.4294, 0.4589, 0.4651, 0.4608, 0.4682,\n",
      "         0.4973, 0.5366, 0.5652, 0.5756, 0.5840, 0.6219, 0.7081, 0.8270, 0.9333,\n",
      "         0.9797, 0.9448, 0.8477, 0.7368, 0.6571, 0.6225, 0.6151, 0.6073, 0.5847,\n",
      "         0.5534, 0.5296, 0.5215],\n",
      "        [0.5399, 0.6335, 0.7970, 0.9299, 0.9747, 0.9238, 0.8035, 0.6678, 0.5725,\n",
      "         0.5403, 0.5482, 0.5551, 0.5369, 0.4987, 0.4631, 0.4479, 0.4505, 0.4542,\n",
      "         0.4456, 0.4260, 0.4075, 0.4006, 0.4042, 0.4081, 0.4044, 0.3944, 0.3863,\n",
      "         0.3859, 0.3905, 0.3923],\n",
      "        [0.4794, 0.4705, 0.4670, 0.4876, 0.5525, 0.6605, 0.7907, 0.9034, 0.9505,\n",
      "         0.9089, 0.8040, 0.6875, 0.6032, 0.5675, 0.5662, 0.5680, 0.5497, 0.5121,\n",
      "         0.4739, 0.4544, 0.4575, 0.4694, 0.4707, 0.4528, 0.4230, 0.3969, 0.3853,\n",
      "         0.3870, 0.3918, 0.3911],\n",
      "        [0.4262, 0.4453, 0.4896, 0.5251, 0.5455, 0.5663, 0.6131, 0.7045, 0.8269,\n",
      "         0.9315, 0.9695, 0.9259, 0.8233, 0.7073, 0.6225, 0.5870, 0.5862, 0.5904,\n",
      "         0.5800, 0.5552, 0.5293, 0.5130, 0.5050, 0.4952, 0.4772, 0.4543, 0.4363,\n",
      "         0.4291, 0.4286, 0.4250],\n",
      "        [0.7874, 0.9215, 0.9831, 0.9224, 0.7962, 0.6632, 0.5624, 0.5132, 0.5069,\n",
      "         0.5121, 0.5014, 0.4709, 0.4361, 0.4154, 0.4144, 0.4217, 0.4215, 0.4073,\n",
      "         0.3866, 0.3716, 0.3685, 0.3722, 0.3725, 0.3645, 0.3527, 0.3462, 0.3492,\n",
      "         0.3573, 0.3612, 0.3560],\n",
      "        [0.0263, 0.0262, 0.0372, 0.0594, 0.0890, 0.1223, 0.1539, 0.1760, 0.1808,\n",
      "         0.1667, 0.1420, 0.1195, 0.1107, 0.1198, 0.1414, 0.1614, 0.1654, 0.1481,\n",
      "         0.1173, 0.0912, 0.0899, 0.1235, 0.1836, 0.2467, 0.2862, 0.2845, 0.2413,\n",
      "         0.1747, 0.1103, 0.0666],\n",
      "        [0.9716, 0.9185, 0.7651, 0.6089, 0.5223, 0.4952, 0.4911, 0.4807, 0.4547,\n",
      "         0.4235, 0.4030, 0.3980, 0.4000, 0.3978, 0.3863, 0.3703, 0.3581, 0.3534,\n",
      "         0.3531, 0.3520, 0.3479, 0.3427, 0.3390, 0.3358, 0.3293, 0.3174, 0.3034,\n",
      "         0.2946, 0.2955, 0.3036],\n",
      "        [0.4951, 0.4809, 0.5265, 0.6395, 0.7840, 0.9046, 0.9514, 0.9025, 0.7802,\n",
      "         0.6454, 0.5550, 0.5232, 0.5264, 0.5309, 0.5177, 0.4920, 0.4722, 0.4695,\n",
      "         0.4789, 0.4862, 0.4804, 0.4626, 0.4425, 0.4280, 0.4187, 0.4089, 0.3956,\n",
      "         0.3825, 0.3761, 0.3785]])\n"
     ]
    }
   ],
   "source": [
    "# First, let's get the features of our bag of words sans positional encoding\n",
    "no_pos = getattr(model.transformer_encoder.layers, '0').norm1((model.encoder(leaked_tokens) * math.sqrt(model.d_model))).cpu()\n",
    "with_pos = recs # Here are those same features, but with positional encodings (stuff we reconstructed)\n",
    "\n",
    "import numpy as np\n",
    "indcs = []\n",
    "corrs = torch.zeros((len(no_pos), len(with_pos))) \n",
    "\n",
    "# We need to find out what word led to what positionally encoded representation. \n",
    "# Let's try the naive greedy search for correlations between no_pos and with_pos as defined above\n",
    "for i, no_p in enumerate(no_pos):\n",
    "    max_corr = 0\n",
    "    for j, with_p in enumerate(with_pos):\n",
    "        val = np.corrcoef(np.array([no_p.detach().numpy(), with_p.detach().numpy()]))[0,1]\n",
    "        corrs[i,j] = val\n",
    "\n",
    "# Greedy search to find which positionally-encoded vector associates with un-positionally-encoded vector\n",
    "order = []\n",
    "for i in range(len(no_pos)):\n",
    "    row, col = torch.argmax(corrs) // corrs.shape[1], torch.argmax(corrs) % corrs.shape[1]\n",
    "    order.append((row.item(), col.item()))\n",
    "    corrs[row,:] = -1\n",
    "    corrs[:, col] = -1\n",
    "\n",
    "order = sorted(order, key=lambda x: x[1])\n",
    "\n",
    "# Now let's re-sort the tokens by this order\n",
    "sorted_tokens1 = [leaked_tokens[order[i][0]] for i in range(len(order))]\n",
    "\n",
    "# Now that we've 'lined-up' the pos-encoded features with non-pos-encoded features, let's subtract the two\n",
    "# to get some 'faux' positions (layer norm means they aren't exact).\n",
    "estimated_pos = torch.stack([with_pos[order[i][1]] - no_pos[order[i][0]] for i in range(len(order))])\n",
    "\n",
    "# Now let's get just the additive part of the positional encoding\n",
    "just_pos = torch.stack([getattr(model.transformer_encoder.layers, '0').norm1(model.pos_encoder(torch.zeros_like(model.encoder(inputs)) * math.sqrt(model.d_model)))]).cpu().squeeze()\n",
    "\n",
    "# We'll do another greedy search, but now it's on the positions of the tokens\n",
    "# First calculate the correlations between (faux) estimated_pos and the (real) just_pos terms\n",
    "order_coeffs = torch.zeros((len(estimated_pos), len(just_pos)))\n",
    "for i in range(len(estimated_pos)):\n",
    "    for j in range(len(just_pos)):\n",
    "        order_coeffs[i,j] = np.corrcoef(estimated_pos[i].detach().numpy(), just_pos[j].detach().numpy())[0,1]\n",
    "\n",
    "print(order_coeffs)\n",
    "\n",
    "# Greedily gobble up the most correlated positions :) \n",
    "pos_order = []\n",
    "for i in range(len(estimated_pos)):\n",
    "    row, col = torch.argmax(order_coeffs) // order_coeffs.shape[1], torch.argmax(order_coeffs) % order_coeffs.shape[1]\n",
    "    pos_order.append((row.item(), col.item()))\n",
    "    order_coeffs[row,:] = -1\n",
    "    order_coeffs[:, col] = -1\n",
    "\n",
    "# Finally, reorder one more time ...\n",
    "pos_order = sorted(pos_order, key=lambda x: x[1])\n",
    "final_sorted_tokens = [sorted_tokens1[pos_order[i][0]] for i in range(len(pos_order))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8f558",
   "metadata": {},
   "source": [
    "### Let's see how we did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065ccaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '<unk>', 'raven', '.', 'the', 'game']\n",
      "['penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'second', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'imperial', 'europan', '<unk>', 'raven', '.', 'game']\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(vocab.lookup_tokens([x[0] for x in inputs]))) # The true sentence\n",
    "print(' '.join(vocab.lookup_tokens(final_sorted_tokens))) # What we reconstruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
