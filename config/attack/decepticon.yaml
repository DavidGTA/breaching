type: analytic

attack_type: decepticon-readout
text_strategy: no-preprocessing # Do not cut off the embedding
label_strategy: # Labels are not required for this attack, but see tokens on step below:

# Key hyperparameters:
token_strategy: decoder-bias # This is actually the "token" recovery strategy as labels~=tokens
token_cutoff: 1.5 # if the token strategy is "embedding-norm" and tied embeddings exist, then this is the cutoff
embedding_token_weight: 0 # Use greedy correlations to the weight embedding in addition to leaked tokens with this weight
sentence_algorithm: dynamic-threshold

# experimental hyperparameters:
# Dont worry about these for normal stuff
normalize_gradients: False
sort_by_bias: False
undivided: False
separation: subtraction # alternative: decorrelation
backfilling: global

impl:
  dtype: float
  mixed_precision: False
  JIT: # bembel with care
