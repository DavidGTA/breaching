
type: see-through-gradients

attack_type: optimization   # analytic, equation solver

objective:
  type: euclidean
  scale: 2555.7032  # This is 0.0001 from the original paper times the number of parameters to recover the correct scaling

restarts:
  num_trials: 1
  scoring: euclidean #'registered-mean' # todo: implement this option

init: randn

optim:
  optimizer: adam
  signed: False
  step_size: 0.1
  boxed: True
  max_iterations: 20_000
  step_size_decay: cosine-decay
  langevin_noise: 0.2
  warmup: 50

  callback: 1000  # Print objective value every callback many iterations

regularization:
  total_variation:
    scale: 1e-4
    inner_exp: 1
    outer_exp: 1
  orthogonality:
    scale: 0.0
  norm:
    scale: 1e-6
    pnorm: 2
  deep_inversion:  # This is batchnorm matching to buffers provided by the user [which can be either actual stats or global]
    scale: 0.1
  # group_regularization: # Not implemented. Unclear how this was done without accesss to the source code
  #  scale: 0.01
