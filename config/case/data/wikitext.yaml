name: wikitext
modality: text
task: causal-lm # next-word / masked-lm / causal-lm

path: "~/data"
size: 1_801_350
shape:
  - 32 # This is sequence_length

# Preprocessing
tokenizer: GPT-2
vocab_size: 50257

# Federated Learning specifics:
default_clients: 29337 # estimate on number of articles in dataset
partition: given # use natural data partition
examples_from_split: training

# Data-specific implementation constants:
batch_size: 128
caching: False
defaults:
  - db: none # Database Setup # use the cmd-line to activate the LMDB module with data.db=LMDB
